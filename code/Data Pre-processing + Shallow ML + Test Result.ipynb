{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: jieba in /Users/andyzhang/anaconda3/lib/python3.7/site-packages (0.42.1)\n",
      "Requirement already satisfied: paddlepaddle-tiny==1.6.1 in /Users/andyzhang/anaconda3/lib/python3.7/site-packages (1.6.1)\n",
      "Requirement already satisfied: objgraph in /Users/andyzhang/anaconda3/lib/python3.7/site-packages (from paddlepaddle-tiny==1.6.1) (3.5.0)\n",
      "Requirement already satisfied: protobuf>=3.1.0 in /Users/andyzhang/anaconda3/lib/python3.7/site-packages (from paddlepaddle-tiny==1.6.1) (3.11.3)\n",
      "Requirement already satisfied: decorator in /Users/andyzhang/anaconda3/lib/python3.7/site-packages (from paddlepaddle-tiny==1.6.1) (4.4.0)\n",
      "Requirement already satisfied: numpy>=1.12 in /Users/andyzhang/anaconda3/lib/python3.7/site-packages (from paddlepaddle-tiny==1.6.1) (1.19.5)\n",
      "Requirement already satisfied: six in /Users/andyzhang/anaconda3/lib/python3.7/site-packages (from paddlepaddle-tiny==1.6.1) (1.12.0)\n",
      "Requirement already satisfied: requests>=2.0.0 in /Users/andyzhang/anaconda3/lib/python3.7/site-packages (from paddlepaddle-tiny==1.6.1) (2.23.0)\n",
      "Requirement already satisfied: graphviz in /Users/andyzhang/anaconda3/lib/python3.7/site-packages (from paddlepaddle-tiny==1.6.1) (0.16)\n",
      "Requirement already satisfied: setuptools in /Users/andyzhang/anaconda3/lib/python3.7/site-packages (from protobuf>=3.1.0->paddlepaddle-tiny==1.6.1) (41.0.1)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /Users/andyzhang/anaconda3/lib/python3.7/site-packages (from requests>=2.0.0->paddlepaddle-tiny==1.6.1) (3.0.4)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /Users/andyzhang/anaconda3/lib/python3.7/site-packages (from requests>=2.0.0->paddlepaddle-tiny==1.6.1) (1.25.11)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/andyzhang/anaconda3/lib/python3.7/site-packages (from requests>=2.0.0->paddlepaddle-tiny==1.6.1) (2019.6.16)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /Users/andyzhang/anaconda3/lib/python3.7/site-packages (from requests>=2.0.0->paddlepaddle-tiny==1.6.1) (2.8)\n",
      "Requirement already satisfied: stopwordsiso in /Users/andyzhang/anaconda3/lib/python3.7/site-packages (0.6.1)\n",
      "Requirement already satisfied: shap in /Users/andyzhang/anaconda3/lib/python3.7/site-packages (0.39.0)\n",
      "Requirement already satisfied: numba in /Users/andyzhang/anaconda3/lib/python3.7/site-packages (from shap) (0.44.1)\n",
      "Requirement already satisfied: tqdm>4.25.0 in /Users/andyzhang/anaconda3/lib/python3.7/site-packages (from shap) (4.46.1)\n",
      "Requirement already satisfied: scipy in /Users/andyzhang/anaconda3/lib/python3.7/site-packages (from shap) (1.3.0)\n",
      "Requirement already satisfied: slicer==0.0.7 in /Users/andyzhang/anaconda3/lib/python3.7/site-packages (from shap) (0.0.7)\n",
      "Requirement already satisfied: pandas in /Users/andyzhang/anaconda3/lib/python3.7/site-packages (from shap) (0.24.2)\n",
      "Requirement already satisfied: cloudpickle in /Users/andyzhang/anaconda3/lib/python3.7/site-packages (from shap) (1.2.1)\n",
      "Requirement already satisfied: scikit-learn in /Users/andyzhang/anaconda3/lib/python3.7/site-packages (from shap) (0.24.1)\n",
      "Requirement already satisfied: numpy in /Users/andyzhang/anaconda3/lib/python3.7/site-packages (from shap) (1.19.5)\n",
      "Requirement already satisfied: llvmlite>=0.29.0 in /Users/andyzhang/anaconda3/lib/python3.7/site-packages (from numba->shap) (0.29.0)\n",
      "Requirement already satisfied: python-dateutil>=2.5.0 in /Users/andyzhang/anaconda3/lib/python3.7/site-packages (from pandas->shap) (2.8.0)\n",
      "Requirement already satisfied: pytz>=2011k in /Users/andyzhang/anaconda3/lib/python3.7/site-packages (from pandas->shap) (2019.1)\n",
      "Requirement already satisfied: six>=1.5 in /Users/andyzhang/anaconda3/lib/python3.7/site-packages (from python-dateutil>=2.5.0->pandas->shap) (1.12.0)\n",
      "Requirement already satisfied: joblib>=0.11 in /Users/andyzhang/anaconda3/lib/python3.7/site-packages (from scikit-learn->shap) (0.13.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /Users/andyzhang/anaconda3/lib/python3.7/site-packages (from scikit-learn->shap) (2.1.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The gensim.similarities.levenshtein submodule is disabled, because the optional Levenshtein package <https://pypi.org/project/python-Levenshtein/> is unavailable. Install Levenhstein (e.g. `pip install python-Levenshtein`) to suppress this warning.\n",
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>.container { width:95% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "!pip install jieba\n",
    "!pip install paddlepaddle-tiny==1.6.1\n",
    "!pip install stopwordsiso\n",
    "!pip install shap\n",
    "\n",
    "'''General'''\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import stopwordsiso\n",
    "from stopwordsiso import stopwords\n",
    "import jieba\n",
    "import jieba.posseg as pseg\n",
    "import time\n",
    "import collections\n",
    "\n",
    "'''Features'''\n",
    "import shap \n",
    "from scipy.sparse import hstack\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "'''Classifiers'''\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "'''CNN'''\n",
    "import gensim\n",
    "import tensorflow as tf\n",
    "from gensim.models import Word2Vec\n",
    "from keras.utils import to_categorical\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras import Sequential, regularizers\n",
    "from tensorflow.keras.layers import Input, Embedding, Conv1D, MaxPooling1D, Flatten, Dense, Dropout, BatchNormalization, concatenate\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "'''Metrics/Evaluation'''\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_curve, auc, confusion_matrix, classification_report\n",
    "\n",
    "'''Display'''\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:95% !important; }</style>\"))\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "pd.options.display.float_format = '{:,.4f}'.format\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "depressed = pd.read_json('depressed.json')\n",
    "normal = pd.read_json('normal.json')\n",
    "normal = normal.loc[normal['label']==0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10325, 11)\n",
      "(10848, 11)\n"
     ]
    }
   ],
   "source": [
    "print(depressed.shape)\n",
    "print(normal.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>all_tweet_count</th>\n",
       "      <th>birthday</th>\n",
       "      <th>gender</th>\n",
       "      <th>label</th>\n",
       "      <th>nickname</th>\n",
       "      <th>num_of_follower</th>\n",
       "      <th>num_of_following</th>\n",
       "      <th>original_tweet_count</th>\n",
       "      <th>profile</th>\n",
       "      <th>repost_tweet_count</th>\n",
       "      <th>tweets</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10</td>\n",
       "      <td>1993-12-10</td>\n",
       "      <td>男</td>\n",
       "      <td>1</td>\n",
       "      <td>迷失路径</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>此人严重丧，不适绕行，谢谢</td>\n",
       "      <td>0</td>\n",
       "      <td>[{'tweet_content': '有点累想休息了', 'posting_time': ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>219</td>\n",
       "      <td>1997-06-07</td>\n",
       "      <td>女</td>\n",
       "      <td>1</td>\n",
       "      <td>九七97吖</td>\n",
       "      <td>173</td>\n",
       "      <td>76</td>\n",
       "      <td>79</td>\n",
       "      <td>无</td>\n",
       "      <td>70</td>\n",
       "      <td>[{'tweet_content': '轉發微博', 'posting_time': '20...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6188</td>\n",
       "      <td>无</td>\n",
       "      <td>女</td>\n",
       "      <td>1</td>\n",
       "      <td>姑先十八-</td>\n",
       "      <td>22</td>\n",
       "      <td>205</td>\n",
       "      <td>24</td>\n",
       "      <td>喜提呼伦贝尔大草原五个月。</td>\n",
       "      <td>5</td>\n",
       "      <td>[{'tweet_content': '抑郁症的普及它好就好在 以前人抑郁的时候想死 现在人...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>62</td>\n",
       "      <td>无</td>\n",
       "      <td>女</td>\n",
       "      <td>1</td>\n",
       "      <td>大邱百香果</td>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "      <td>17</td>\n",
       "      <td>无</td>\n",
       "      <td>6</td>\n",
       "      <td>[{'tweet_content': '', 'posting_time': '2017-0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6</td>\n",
       "      <td>2001-09-25</td>\n",
       "      <td>女</td>\n",
       "      <td>1</td>\n",
       "      <td>落墨暮烟</td>\n",
       "      <td>1</td>\n",
       "      <td>27</td>\n",
       "      <td>6</td>\n",
       "      <td>无</td>\n",
       "      <td>0</td>\n",
       "      <td>[{'tweet_content': '所有人都问我你没事吧你还好吧 我笑着一一回答我很好啊...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   all_tweet_count    birthday gender  label nickname  num_of_follower  \\\n",
       "0               10  1993-12-10      男      1     迷失路径                1   \n",
       "1              219  1997-06-07      女      1    九七97吖              173   \n",
       "2             6188           无      女      1    姑先十八-               22   \n",
       "3               62           无      女      1    大邱百香果                2   \n",
       "4                6  2001-09-25      女      1     落墨暮烟                1   \n",
       "\n",
       "   num_of_following  original_tweet_count        profile  repost_tweet_count  \\\n",
       "0                 0                    10  此人严重丧，不适绕行，谢谢                   0   \n",
       "1                76                    79              无                  70   \n",
       "2               205                    24  喜提呼伦贝尔大草原五个月。                   5   \n",
       "3                 9                    17              无                   6   \n",
       "4                27                     6              无                   0   \n",
       "\n",
       "                                              tweets  \n",
       "0  [{'tweet_content': '有点累想休息了', 'posting_time': ...  \n",
       "1  [{'tweet_content': '轉發微博', 'posting_time': '20...  \n",
       "2  [{'tweet_content': '抑郁症的普及它好就好在 以前人抑郁的时候想死 现在人...  \n",
       "3  [{'tweet_content': '', 'posting_time': '2017-0...  \n",
       "4  [{'tweet_content': '所有人都问我你没事吧你还好吧 我笑着一一回答我很好啊...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "depressed.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>all_tweet_count</th>\n",
       "      <th>birthday</th>\n",
       "      <th>gender</th>\n",
       "      <th>label</th>\n",
       "      <th>nickname</th>\n",
       "      <th>num_of_follower</th>\n",
       "      <th>num_of_following</th>\n",
       "      <th>original_tweet_count</th>\n",
       "      <th>profile</th>\n",
       "      <th>repost_tweet_count</th>\n",
       "      <th>tweets</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2877</td>\n",
       "      <td>2000-08-02</td>\n",
       "      <td>女</td>\n",
       "      <td>0</td>\n",
       "      <td>心无挂碍wxq</td>\n",
       "      <td>581</td>\n",
       "      <td>968</td>\n",
       "      <td>98</td>\n",
       "      <td>传播正能量，广结善缘。每天懂一点佛学知识，感悟人生，净化身心，弘扬佛法，传播中国传统文化，便...</td>\n",
       "      <td>0</td>\n",
       "      <td>[{'tweet_content': '无', 'posting_time': '2020-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1326</td>\n",
       "      <td>1998-04-10</td>\n",
       "      <td>女</td>\n",
       "      <td>0</td>\n",
       "      <td>透心凉A兔子</td>\n",
       "      <td>1363</td>\n",
       "      <td>241</td>\n",
       "      <td>97</td>\n",
       "      <td>无</td>\n",
       "      <td>1</td>\n",
       "      <td>[{'tweet_content': '单身狗不能接受的路过 还有谁不知道里的求婚场面今晚的...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1883</td>\n",
       "      <td>无</td>\n",
       "      <td>男</td>\n",
       "      <td>0</td>\n",
       "      <td>路路090909</td>\n",
       "      <td>141</td>\n",
       "      <td>1480</td>\n",
       "      <td>62</td>\n",
       "      <td>所谓活着的人，就是不断挑战的人，不断攀登命运峻峰的人。</td>\n",
       "      <td>38</td>\n",
       "      <td>[{'tweet_content': '我不喜欢这世界我只喜欢你', 'posting_ti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>398</td>\n",
       "      <td>无</td>\n",
       "      <td>女</td>\n",
       "      <td>0</td>\n",
       "      <td>Yuancuxin</td>\n",
       "      <td>6649</td>\n",
       "      <td>130</td>\n",
       "      <td>94</td>\n",
       "      <td>浪漫至死不渝?</td>\n",
       "      <td>6</td>\n",
       "      <td>[{'tweet_content': '大抵是因为身边人都很温柔 所以我经不起一点点凶 嘿嘿...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>258</td>\n",
       "      <td>无</td>\n",
       "      <td>女</td>\n",
       "      <td>0</td>\n",
       "      <td>梦醒泪落88</td>\n",
       "      <td>1400</td>\n",
       "      <td>863</td>\n",
       "      <td>86</td>\n",
       "      <td>88年老阿姨！！私有财产两个儿子??</td>\n",
       "      <td>4</td>\n",
       "      <td>[{'tweet_content': '快手千万现金红包等你来拿', 'posting_ti...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   all_tweet_count    birthday gender  label   nickname  num_of_follower  \\\n",
       "1             2877  2000-08-02      女      0    心无挂碍wxq              581   \n",
       "2             1326  1998-04-10      女      0     透心凉A兔子             1363   \n",
       "4             1883           无      男      0   路路090909              141   \n",
       "6              398           无      女      0  Yuancuxin             6649   \n",
       "7              258           无      女      0     梦醒泪落88             1400   \n",
       "\n",
       "   num_of_following  original_tweet_count  \\\n",
       "1               968                    98   \n",
       "2               241                    97   \n",
       "4              1480                    62   \n",
       "6               130                    94   \n",
       "7               863                    86   \n",
       "\n",
       "                                             profile  repost_tweet_count  \\\n",
       "1  传播正能量，广结善缘。每天懂一点佛学知识，感悟人生，净化身心，弘扬佛法，传播中国传统文化，便...                   0   \n",
       "2                                                  无                   1   \n",
       "4                        所谓活着的人，就是不断挑战的人，不断攀登命运峻峰的人。                  38   \n",
       "6                                            浪漫至死不渝?                   6   \n",
       "7                                 88年老阿姨！！私有财产两个儿子??                   4   \n",
       "\n",
       "                                              tweets  \n",
       "1  [{'tweet_content': '无', 'posting_time': '2020-...  \n",
       "2  [{'tweet_content': '单身狗不能接受的路过 还有谁不知道里的求婚场面今晚的...  \n",
       "4  [{'tweet_content': '我不喜欢这世界我只喜欢你', 'posting_ti...  \n",
       "6  [{'tweet_content': '大抵是因为身边人都很温柔 所以我经不起一点点凶 嘿嘿...  \n",
       "7  [{'tweet_content': '快手千万现金红包等你来拿', 'posting_ti...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normal.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "all_tweet_count         0\n",
       "birthday                0\n",
       "gender                  0\n",
       "label                   0\n",
       "nickname                0\n",
       "num_of_follower         0\n",
       "num_of_following        0\n",
       "original_tweet_count    0\n",
       "profile                 0\n",
       "repost_tweet_count      0\n",
       "tweets                  0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "depressed.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "all_tweet_count         0\n",
       "birthday                0\n",
       "gender                  0\n",
       "label                   0\n",
       "nickname                0\n",
       "num_of_follower         0\n",
       "num_of_following        0\n",
       "original_tweet_count    0\n",
       "profile                 0\n",
       "repost_tweet_count      0\n",
       "tweets                  0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normal.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "797"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "STOPWORDS = stopwords([\"zh\"])  # Chinese\n",
    "STOPWORDS.update({\"轉發微博\", \"轉發\", \"微博\"})\n",
    "len(STOPWORDS)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def isChinese(uchar):\n",
    "    if uchar >= u'\\u4e00' and uchar <= u'\\u9fa5':\n",
    "        return uchar\n",
    "    else:\n",
    "        return ' '\n",
    "\n",
    "def format_str(content):\n",
    "    content_str = ''\n",
    "    for i in content:\n",
    "        content_str = content_str + isChinese(i)\n",
    "    return content_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install emoji\n",
    "# from emoji import UNICODE_EMOJI\n",
    "\n",
    "# def is_emoji(s):\n",
    "#     return s in UNICODE_EMOJI\n",
    "\n",
    "# def count_emoji(content):\n",
    "#     content_str = ''\n",
    "#     for i in content:\n",
    "#         if i in UNICODE_EMOJI:\n",
    "#             content_str = content_str + i\n",
    "#     return content_str"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Pre-processing\n",
    "- Data cleaning\n",
    "- Feature engineering\n",
    "- EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Paddle enabled successfully......\n"
     ]
    }
   ],
   "source": [
    "jieba.enable_paddle() \n",
    "# jieba.enable_parallel() will cause error if enabled here\n",
    "\n",
    "def clean_data(df):\n",
    "    df = df[['tweets', 'label']]\n",
    "    # df = df[['tweets', 'label']].sample(1000, random_state=1)\n",
    "\n",
    "    # extract all post content into flattern array\n",
    "    df['tweets_flat'] = df['tweets'].apply(lambda tweets: ' '.join([tweet['tweet_content'] for tweet in tweets]))\n",
    "    # remove alphanumerical and special chars, keep only Chinese\n",
    "    df['tweets_chinese'] = df['tweets_flat'].apply(lambda tweets: format_str(tweets))\n",
    "    # text segmentation using JIEBA, paddle mode\n",
    "    df['tweets_cut'] = df['tweets_chinese'].apply(lambda t: ' '.join(list(jieba.cut(t, \n",
    "                                                                                use_paddle=True\n",
    "    #                                                                           cut_all=True\n",
    "                                                                                ))))\n",
    "    # remove stopwords\n",
    "    df['tweets_clean'] = df['tweets_cut'].apply(lambda t: ' '.join([word for word in t.split(' ') if word not in STOPWORDS ]))\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "pos_cols = ['n', 'f', 's', 't', 'nr', 'ns', 'nt', 'nw', 'nz', 'v', 'vd', 'vn', 'a', 'ad', 'an', 'd', 'm', 'q', 'r', 'p', 'c', 'u', 'xc', 'w', 'PER', 'LOC', 'ORG', 'TIME', 'O']\n",
    "\n",
    "# generate pos tag features\n",
    "def generate_pos(df):   \n",
    "    df = df.join(pd.DataFrame(\n",
    "        [[0]*29], \n",
    "        index=df.index, \n",
    "        columns=pos_cols\n",
    "    ))\n",
    "    for index, row in df.iterrows():\n",
    "        words = pseg.cut(row['tweets_chinese'],use_paddle=True)\n",
    "        for word, flag in words:\n",
    "            row[flag] += 1\n",
    "        df.at[index] = row\n",
    "        \n",
    "    return df\n",
    "\n",
    "\n",
    "sentiment_dict = \"BosonNLP_sentiment_score.txt\"\n",
    "with open(sentiment_dict) as f:\n",
    "    boson_dict = f.read().splitlines() \n",
    "\n",
    "# generate pos tag features\n",
    "def generate_sentiment(df):\n",
    "    pos = []\n",
    "    neg = []\n",
    "    \n",
    "    # sentiment threshold, the greater the value, the stronger the emotion\n",
    "    threshold = 1\n",
    "\n",
    "    # chars to be removed in sentiment dict\n",
    "    pattern = re.compile(\"[A-Za-z0-9.·:_=／￥—@#%\\-\\+\\\\\\]+\")\n",
    "\n",
    "    # filter dict by threhold and pattern\n",
    "    for w in boson_dict:\n",
    "        if w == '':\n",
    "            pass\n",
    "        else:\n",
    "            word = w.split(' ')[0]\n",
    "            value = w.split(' ')[1]\n",
    "            if float(value) >= threshold and not pattern.match(word):\n",
    "                pos.append(word)\n",
    "            elif float(value) <= threshold and not pattern.match(word):\n",
    "                neg.append(word)\n",
    "#     print('positive length: %d, negative length: %d' %(len(pos),len(neg)))\n",
    "\n",
    "    df = df.drop(['n_pos','n_neg'], axis=1, errors='ignore')\n",
    "    df['n_pos'] = df['tweets_clean'].apply(lambda x: np.isin(list(filter(str.strip,x.split(\" \"))), pos).sum())\n",
    "    df['n_neg'] = df['tweets_clean'].apply(lambda x: np.isin(list(filter(str.strip,x.split(\" \"))), neg).sum())\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(21173, 11)\n"
     ]
    }
   ],
   "source": [
    "users = pd.concat([depressed, normal], axis=0)\n",
    "print(users.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "progress: 14200\n",
      "progress: 15200\n",
      "progress: 16200\n",
      "progress: 17200\n",
      "progress: 18200\n",
      "progress: 19200\n",
      "progress: 20200\n"
     ]
    }
   ],
   "source": [
    "# process in chunk\n",
    "# if load all data into memory and run pre-process will cause kernel died\n",
    "\n",
    "start = 0\n",
    "offset = 1000\n",
    "\n",
    "while start < 21173: \n",
    "    print('progress:', start)\n",
    "    \n",
    "    limit = start + offset\n",
    "    df = clean_data(users.iloc[start:limit])\n",
    "    df = generate_pos(df)\n",
    "    df = generate_sentiment(df)\n",
    "    df.to_csv(\"cleaned_\"+str(start)+'-'+str(limit)+'.csv', index=False)\n",
    "    \n",
    "    start = limit\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweets</th>\n",
       "      <th>label</th>\n",
       "      <th>tweets_flat</th>\n",
       "      <th>tweets_chinese</th>\n",
       "      <th>tweets_cut</th>\n",
       "      <th>tweets_clean</th>\n",
       "      <th>n</th>\n",
       "      <th>f</th>\n",
       "      <th>s</th>\n",
       "      <th>t</th>\n",
       "      <th>nr</th>\n",
       "      <th>ns</th>\n",
       "      <th>nt</th>\n",
       "      <th>nw</th>\n",
       "      <th>nz</th>\n",
       "      <th>v</th>\n",
       "      <th>vd</th>\n",
       "      <th>vn</th>\n",
       "      <th>a</th>\n",
       "      <th>ad</th>\n",
       "      <th>an</th>\n",
       "      <th>d</th>\n",
       "      <th>m</th>\n",
       "      <th>q</th>\n",
       "      <th>r</th>\n",
       "      <th>p</th>\n",
       "      <th>c</th>\n",
       "      <th>u</th>\n",
       "      <th>xc</th>\n",
       "      <th>w</th>\n",
       "      <th>PER</th>\n",
       "      <th>LOC</th>\n",
       "      <th>ORG</th>\n",
       "      <th>TIME</th>\n",
       "      <th>O</th>\n",
       "      <th>n_pos</th>\n",
       "      <th>n_neg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[{'tweet_content': '请多多注意防护预防感染常戴口罩', 'posting...</td>\n",
       "      <td>0</td>\n",
       "      <td>请多多注意防护预防感染常戴口罩 走在乡间的小路上乡村风景美如画 空运的螃蟹到了快来领取啊 何...</td>\n",
       "      <td>请多多注意防护预防感染常戴口罩 走在乡间的小路上乡村风景美如画 空运的螃蟹到了快来领取啊 何...</td>\n",
       "      <td>请 多多 注意 防护 预防 感染 常 戴口罩   走 在 乡间 的 小路 上 乡村 风景 美...</td>\n",
       "      <td>多多 注意 防护 预防 感染 常 戴口罩   走 乡间 小路 乡村 风景 美如画 空运 螃蟹...</td>\n",
       "      <td>562</td>\n",
       "      <td>47</td>\n",
       "      <td>18</td>\n",
       "      <td>16</td>\n",
       "      <td>47</td>\n",
       "      <td>3</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>48</td>\n",
       "      <td>763</td>\n",
       "      <td>6</td>\n",
       "      <td>67</td>\n",
       "      <td>168</td>\n",
       "      <td>14</td>\n",
       "      <td>9</td>\n",
       "      <td>202</td>\n",
       "      <td>79</td>\n",
       "      <td>13</td>\n",
       "      <td>296</td>\n",
       "      <td>125</td>\n",
       "      <td>82</td>\n",
       "      <td>337</td>\n",
       "      <td>36</td>\n",
       "      <td>2</td>\n",
       "      <td>24</td>\n",
       "      <td>109</td>\n",
       "      <td>13</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>330</td>\n",
       "      <td>1095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[{'tweet_content': '你敢不敢让我中个小恐龙', 'posting_tim...</td>\n",
       "      <td>0</td>\n",
       "      <td>你敢不敢让我中个小恐龙 周年庆邀你盛装出席 5月1日5月8日参与周年庆活动分享你的周年庆时装...</td>\n",
       "      <td>你敢不敢让我中个小恐龙 周年庆邀你盛装出席  月 日 月 日参与周年庆活动分享你的周年庆时装...</td>\n",
       "      <td>你 敢不敢 让 我 中 个 小 恐龙   周年庆 邀 你 盛装 出席   月 日 月 日 参...</td>\n",
       "      <td>敢不敢 恐龙   周年庆 邀 盛装 出席   参与 周年庆 活动 分享 周年庆 时装 穿 搭...</td>\n",
       "      <td>364</td>\n",
       "      <td>24</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>67</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>645</td>\n",
       "      <td>1</td>\n",
       "      <td>18</td>\n",
       "      <td>126</td>\n",
       "      <td>14</td>\n",
       "      <td>3</td>\n",
       "      <td>194</td>\n",
       "      <td>44</td>\n",
       "      <td>21</td>\n",
       "      <td>223</td>\n",
       "      <td>50</td>\n",
       "      <td>36</td>\n",
       "      <td>175</td>\n",
       "      <td>76</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>41</td>\n",
       "      <td>0</td>\n",
       "      <td>151</td>\n",
       "      <td>702</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[{'tweet_content': '滴 613打卡成功 天气好的一天 看见小泽自拍 心情...</td>\n",
       "      <td>0</td>\n",
       "      <td>滴 613打卡成功 天气好的一天 看见小泽自拍 心情更加好 以后多发呗 早上好呀天泽 天气好...</td>\n",
       "      <td>滴    打卡成功 天气好的一天 看见小泽自拍 心情更加好 以后多发呗 早上好呀天泽 天气好...</td>\n",
       "      <td>滴     打卡 成功   天气 好 的 一天   看见 小泽 自拍   心情 更加 好  ...</td>\n",
       "      <td>滴     打卡 成功   天气 一天   看见 小泽 自拍   心情 更加  以后 多发 ...</td>\n",
       "      <td>81</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>116</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>38</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>22</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>31</td>\n",
       "      <td>28</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>28</td>\n",
       "      <td>0</td>\n",
       "      <td>54</td>\n",
       "      <td>123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[{'tweet_content': '有没有人跟我一样的卡了我2天52个精英令用都用不鸟这...</td>\n",
       "      <td>0</td>\n",
       "      <td>有没有人跟我一样的卡了我2天52个精英令用都用不鸟这波过去不卸载我是 我在打卡啦每日签到领红...</td>\n",
       "      <td>有没有人跟我一样的卡了我 天  个精英令用都用不鸟这波过去不卸载我是 我在打卡啦每日签到领红...</td>\n",
       "      <td>有没有 人 跟 我 一样 的 卡 了 我  天   个 精英令 用 都 用 不 鸟 这 波 ...</td>\n",
       "      <td>有没有 卡  天   精英令 鸟 波 过去 卸载   打卡 每日 签到 领 红包 签到 越多...</td>\n",
       "      <td>292</td>\n",
       "      <td>6</td>\n",
       "      <td>9</td>\n",
       "      <td>5</td>\n",
       "      <td>32</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "      <td>450</td>\n",
       "      <td>2</td>\n",
       "      <td>23</td>\n",
       "      <td>84</td>\n",
       "      <td>9</td>\n",
       "      <td>13</td>\n",
       "      <td>111</td>\n",
       "      <td>35</td>\n",
       "      <td>9</td>\n",
       "      <td>142</td>\n",
       "      <td>52</td>\n",
       "      <td>19</td>\n",
       "      <td>143</td>\n",
       "      <td>52</td>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>159</td>\n",
       "      <td>490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[{'tweet_content': '婚姻不分年龄希望决定结婚之前能够想清楚对方是你想要相...</td>\n",
       "      <td>0</td>\n",
       "      <td>婚姻不分年龄希望决定结婚之前能够想清楚对方是你想要相伴一生的人而且不管前路好坏面对诱惑时你还...</td>\n",
       "      <td>婚姻不分年龄希望决定结婚之前能够想清楚对方是你想要相伴一生的人而且不管前路好坏面对诱惑时你还...</td>\n",
       "      <td>婚姻 不分 年龄 希望 决定 结婚之前 能够 想清楚 对方 是 你 想要 相伴 一生 的 人...</td>\n",
       "      <td>婚姻 不分 年龄 希望 决定 结婚之前 能够 想清楚 想要 相伴 一生 前路 好坏 面对 诱...</td>\n",
       "      <td>341</td>\n",
       "      <td>23</td>\n",
       "      <td>4</td>\n",
       "      <td>18</td>\n",
       "      <td>23</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>632</td>\n",
       "      <td>3</td>\n",
       "      <td>37</td>\n",
       "      <td>124</td>\n",
       "      <td>17</td>\n",
       "      <td>11</td>\n",
       "      <td>232</td>\n",
       "      <td>44</td>\n",
       "      <td>7</td>\n",
       "      <td>227</td>\n",
       "      <td>67</td>\n",
       "      <td>54</td>\n",
       "      <td>187</td>\n",
       "      <td>22</td>\n",
       "      <td>0</td>\n",
       "      <td>18</td>\n",
       "      <td>27</td>\n",
       "      <td>6</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>230</td>\n",
       "      <td>662</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              tweets  label  \\\n",
       "0  [{'tweet_content': '请多多注意防护预防感染常戴口罩', 'posting...      0   \n",
       "1  [{'tweet_content': '你敢不敢让我中个小恐龙', 'posting_tim...      0   \n",
       "2  [{'tweet_content': '滴 613打卡成功 天气好的一天 看见小泽自拍 心情...      0   \n",
       "3  [{'tweet_content': '有没有人跟我一样的卡了我2天52个精英令用都用不鸟这...      0   \n",
       "4  [{'tweet_content': '婚姻不分年龄希望决定结婚之前能够想清楚对方是你想要相...      0   \n",
       "\n",
       "                                         tweets_flat  \\\n",
       "0  请多多注意防护预防感染常戴口罩 走在乡间的小路上乡村风景美如画 空运的螃蟹到了快来领取啊 何...   \n",
       "1  你敢不敢让我中个小恐龙 周年庆邀你盛装出席 5月1日5月8日参与周年庆活动分享你的周年庆时装...   \n",
       "2  滴 613打卡成功 天气好的一天 看见小泽自拍 心情更加好 以后多发呗 早上好呀天泽 天气好...   \n",
       "3  有没有人跟我一样的卡了我2天52个精英令用都用不鸟这波过去不卸载我是 我在打卡啦每日签到领红...   \n",
       "4  婚姻不分年龄希望决定结婚之前能够想清楚对方是你想要相伴一生的人而且不管前路好坏面对诱惑时你还...   \n",
       "\n",
       "                                      tweets_chinese  \\\n",
       "0  请多多注意防护预防感染常戴口罩 走在乡间的小路上乡村风景美如画 空运的螃蟹到了快来领取啊 何...   \n",
       "1  你敢不敢让我中个小恐龙 周年庆邀你盛装出席  月 日 月 日参与周年庆活动分享你的周年庆时装...   \n",
       "2  滴    打卡成功 天气好的一天 看见小泽自拍 心情更加好 以后多发呗 早上好呀天泽 天气好...   \n",
       "3  有没有人跟我一样的卡了我 天  个精英令用都用不鸟这波过去不卸载我是 我在打卡啦每日签到领红...   \n",
       "4  婚姻不分年龄希望决定结婚之前能够想清楚对方是你想要相伴一生的人而且不管前路好坏面对诱惑时你还...   \n",
       "\n",
       "                                          tweets_cut  \\\n",
       "0  请 多多 注意 防护 预防 感染 常 戴口罩   走 在 乡间 的 小路 上 乡村 风景 美...   \n",
       "1  你 敢不敢 让 我 中 个 小 恐龙   周年庆 邀 你 盛装 出席   月 日 月 日 参...   \n",
       "2  滴     打卡 成功   天气 好 的 一天   看见 小泽 自拍   心情 更加 好  ...   \n",
       "3  有没有 人 跟 我 一样 的 卡 了 我  天   个 精英令 用 都 用 不 鸟 这 波 ...   \n",
       "4  婚姻 不分 年龄 希望 决定 结婚之前 能够 想清楚 对方 是 你 想要 相伴 一生 的 人...   \n",
       "\n",
       "                                        tweets_clean    n   f   s   t  nr  ns  \\\n",
       "0  多多 注意 防护 预防 感染 常 戴口罩   走 乡间 小路 乡村 风景 美如画 空运 螃蟹...  562  47  18  16  47   3   \n",
       "1  敢不敢 恐龙   周年庆 邀 盛装 出席   参与 周年庆 活动 分享 周年庆 时装 穿 搭...  364  24   7   7  67   0   \n",
       "2  滴     打卡 成功   天气 一天   看见 小泽 自拍   心情 更加  以后 多发 ...   81  13   0   0  16   0   \n",
       "3  有没有 卡  天   精英令 鸟 波 过去 卸载   打卡 每日 签到 领 红包 签到 越多...  292   6   9   5  32   0   \n",
       "4  婚姻 不分 年龄 希望 决定 结婚之前 能够 想清楚 想要 相伴 一生 前路 好坏 面对 诱...  341  23   4  18  23   0   \n",
       "\n",
       "   nt  nw  nz    v  vd  vn    a  ad  an    d   m   q    r    p   c    u  xc  \\\n",
       "0   9   1  48  763   6  67  168  14   9  202  79  13  296  125  82  337  36   \n",
       "1   3   0  12  645   1  18  126  14   3  194  44  21  223   50  36  175  76   \n",
       "2   0   0   1  116   1   1   38   2   0   22   2   1   11    3   3   31  28   \n",
       "3   3   0  16  450   2  23   84   9  13  111  35   9  142   52  19  143  52   \n",
       "4   1   0   7  632   3  37  124  17  11  232  44   7  227   67  54  187  22   \n",
       "\n",
       "   w  PER  LOC  ORG  TIME  O  n_pos  n_neg  \n",
       "0  2   24  109   13    14  0    330   1095  \n",
       "1  3    7    8    1    41  0    151    702  \n",
       "2  0   13    0    0    28  0     54    123  \n",
       "3  2    9    2    5    10  0    159    490  \n",
       "4  0   18   27    6    13  0    230    662  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read back chunked results and concate together\n",
    "\n",
    "import glob\n",
    "\n",
    "df_train = pd.DataFrame()\n",
    "for file in glob.glob(\"cleaned_*.csv\"):\n",
    "    df = pd.read_csv(file)\n",
    "    if df_train.empty:\n",
    "        df_train = df\n",
    "    else:\n",
    "        df_train = df_train.append(df)\n",
    "\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(21073, 38)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweets</th>\n",
       "      <th>label</th>\n",
       "      <th>tweets_flat</th>\n",
       "      <th>tweets_chinese</th>\n",
       "      <th>tweets_cut</th>\n",
       "      <th>tweets_clean</th>\n",
       "      <th>n</th>\n",
       "      <th>f</th>\n",
       "      <th>s</th>\n",
       "      <th>t</th>\n",
       "      <th>nr</th>\n",
       "      <th>ns</th>\n",
       "      <th>nt</th>\n",
       "      <th>nw</th>\n",
       "      <th>nz</th>\n",
       "      <th>v</th>\n",
       "      <th>vd</th>\n",
       "      <th>vn</th>\n",
       "      <th>a</th>\n",
       "      <th>ad</th>\n",
       "      <th>an</th>\n",
       "      <th>d</th>\n",
       "      <th>m</th>\n",
       "      <th>q</th>\n",
       "      <th>r</th>\n",
       "      <th>p</th>\n",
       "      <th>c</th>\n",
       "      <th>u</th>\n",
       "      <th>xc</th>\n",
       "      <th>w</th>\n",
       "      <th>PER</th>\n",
       "      <th>LOC</th>\n",
       "      <th>ORG</th>\n",
       "      <th>TIME</th>\n",
       "      <th>O</th>\n",
       "      <th>n_pos</th>\n",
       "      <th>n_neg</th>\n",
       "      <th>word_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[{'tweet_content': '请多多注意防护预防感染常戴口罩', 'posting...</td>\n",
       "      <td>0</td>\n",
       "      <td>请多多注意防护预防感染常戴口罩 走在乡间的小路上乡村风景美如画 空运的螃蟹到了快来领取啊 何...</td>\n",
       "      <td>请多多注意防护预防感染常戴口罩 走在乡间的小路上乡村风景美如画 空运的螃蟹到了快来领取啊 何...</td>\n",
       "      <td>请 多多 注意 防护 预防 感染 常 戴口罩   走 在 乡间 的 小路 上 乡村 风景 美...</td>\n",
       "      <td>多多 注意 防护 预防 感染 常 戴口罩   走 乡间 小路 乡村 风景 美如画 空运 螃蟹...</td>\n",
       "      <td>0.1953</td>\n",
       "      <td>0.0163</td>\n",
       "      <td>0.0063</td>\n",
       "      <td>0.0056</td>\n",
       "      <td>0.0163</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>0.0031</td>\n",
       "      <td>0.0003</td>\n",
       "      <td>0.0167</td>\n",
       "      <td>0.2651</td>\n",
       "      <td>0.0021</td>\n",
       "      <td>0.0233</td>\n",
       "      <td>0.0584</td>\n",
       "      <td>0.0049</td>\n",
       "      <td>0.0031</td>\n",
       "      <td>0.0702</td>\n",
       "      <td>0.0274</td>\n",
       "      <td>0.0045</td>\n",
       "      <td>0.1028</td>\n",
       "      <td>0.0434</td>\n",
       "      <td>0.0285</td>\n",
       "      <td>0.1171</td>\n",
       "      <td>0.0125</td>\n",
       "      <td>0.0007</td>\n",
       "      <td>0.0083</td>\n",
       "      <td>0.0379</td>\n",
       "      <td>0.0045</td>\n",
       "      <td>0.0049</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.1147</td>\n",
       "      <td>0.3805</td>\n",
       "      <td>2878</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[{'tweet_content': '你敢不敢让我中个小恐龙', 'posting_tim...</td>\n",
       "      <td>0</td>\n",
       "      <td>你敢不敢让我中个小恐龙 周年庆邀你盛装出席 5月1日5月8日参与周年庆活动分享你的周年庆时装...</td>\n",
       "      <td>你敢不敢让我中个小恐龙 周年庆邀你盛装出席  月 日 月 日参与周年庆活动分享你的周年庆时装...</td>\n",
       "      <td>你 敢不敢 让 我 中 个 小 恐龙   周年庆 邀 你 盛装 出席   月 日 月 日 参...</td>\n",
       "      <td>敢不敢 恐龙   周年庆 邀 盛装 出席   参与 周年庆 活动 分享 周年庆 时装 穿 搭...</td>\n",
       "      <td>0.1860</td>\n",
       "      <td>0.0123</td>\n",
       "      <td>0.0036</td>\n",
       "      <td>0.0036</td>\n",
       "      <td>0.0342</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0015</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0061</td>\n",
       "      <td>0.3296</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>0.0092</td>\n",
       "      <td>0.0644</td>\n",
       "      <td>0.0072</td>\n",
       "      <td>0.0015</td>\n",
       "      <td>0.0991</td>\n",
       "      <td>0.0225</td>\n",
       "      <td>0.0107</td>\n",
       "      <td>0.1139</td>\n",
       "      <td>0.0255</td>\n",
       "      <td>0.0184</td>\n",
       "      <td>0.0894</td>\n",
       "      <td>0.0388</td>\n",
       "      <td>0.0015</td>\n",
       "      <td>0.0036</td>\n",
       "      <td>0.0041</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>0.0210</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0772</td>\n",
       "      <td>0.3587</td>\n",
       "      <td>1957</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[{'tweet_content': '滴 613打卡成功 天气好的一天 看见小泽自拍 心情...</td>\n",
       "      <td>0</td>\n",
       "      <td>滴 613打卡成功 天气好的一天 看见小泽自拍 心情更加好 以后多发呗 早上好呀天泽 天气好...</td>\n",
       "      <td>滴    打卡成功 天气好的一天 看见小泽自拍 心情更加好 以后多发呗 早上好呀天泽 天气好...</td>\n",
       "      <td>滴     打卡 成功   天气 好 的 一天   看见 小泽 自拍   心情 更加 好  ...</td>\n",
       "      <td>滴     打卡 成功   天气 一天   看见 小泽 自拍   心情 更加  以后 多发 ...</td>\n",
       "      <td>0.2109</td>\n",
       "      <td>0.0339</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0417</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0026</td>\n",
       "      <td>0.3021</td>\n",
       "      <td>0.0026</td>\n",
       "      <td>0.0026</td>\n",
       "      <td>0.0990</td>\n",
       "      <td>0.0052</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0573</td>\n",
       "      <td>0.0052</td>\n",
       "      <td>0.0026</td>\n",
       "      <td>0.0286</td>\n",
       "      <td>0.0078</td>\n",
       "      <td>0.0078</td>\n",
       "      <td>0.0807</td>\n",
       "      <td>0.0729</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0339</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0729</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.1406</td>\n",
       "      <td>0.3203</td>\n",
       "      <td>384</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[{'tweet_content': '有没有人跟我一样的卡了我2天52个精英令用都用不鸟这...</td>\n",
       "      <td>0</td>\n",
       "      <td>有没有人跟我一样的卡了我2天52个精英令用都用不鸟这波过去不卸载我是 我在打卡啦每日签到领红...</td>\n",
       "      <td>有没有人跟我一样的卡了我 天  个精英令用都用不鸟这波过去不卸载我是 我在打卡啦每日签到领红...</td>\n",
       "      <td>有没有 人 跟 我 一样 的 卡 了 我  天   个 精英令 用 都 用 不 鸟 这 波 ...</td>\n",
       "      <td>有没有 卡  天   精英令 鸟 波 过去 卸载   打卡 每日 签到 领 红包 签到 越多...</td>\n",
       "      <td>0.2083</td>\n",
       "      <td>0.0043</td>\n",
       "      <td>0.0064</td>\n",
       "      <td>0.0036</td>\n",
       "      <td>0.0228</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0021</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0114</td>\n",
       "      <td>0.3210</td>\n",
       "      <td>0.0014</td>\n",
       "      <td>0.0164</td>\n",
       "      <td>0.0599</td>\n",
       "      <td>0.0064</td>\n",
       "      <td>0.0093</td>\n",
       "      <td>0.0792</td>\n",
       "      <td>0.0250</td>\n",
       "      <td>0.0064</td>\n",
       "      <td>0.1013</td>\n",
       "      <td>0.0371</td>\n",
       "      <td>0.0136</td>\n",
       "      <td>0.1020</td>\n",
       "      <td>0.0371</td>\n",
       "      <td>0.0014</td>\n",
       "      <td>0.0064</td>\n",
       "      <td>0.0014</td>\n",
       "      <td>0.0036</td>\n",
       "      <td>0.0071</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.1134</td>\n",
       "      <td>0.3495</td>\n",
       "      <td>1402</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[{'tweet_content': '婚姻不分年龄希望决定结婚之前能够想清楚对方是你想要相...</td>\n",
       "      <td>0</td>\n",
       "      <td>婚姻不分年龄希望决定结婚之前能够想清楚对方是你想要相伴一生的人而且不管前路好坏面对诱惑时你还...</td>\n",
       "      <td>婚姻不分年龄希望决定结婚之前能够想清楚对方是你想要相伴一生的人而且不管前路好坏面对诱惑时你还...</td>\n",
       "      <td>婚姻 不分 年龄 希望 决定 结婚之前 能够 想清楚 对方 是 你 想要 相伴 一生 的 人...</td>\n",
       "      <td>婚姻 不分 年龄 希望 决定 结婚之前 能够 想清楚 想要 相伴 一生 前路 好坏 面对 诱...</td>\n",
       "      <td>0.1676</td>\n",
       "      <td>0.0113</td>\n",
       "      <td>0.0020</td>\n",
       "      <td>0.0088</td>\n",
       "      <td>0.0113</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0034</td>\n",
       "      <td>0.3107</td>\n",
       "      <td>0.0015</td>\n",
       "      <td>0.0182</td>\n",
       "      <td>0.0610</td>\n",
       "      <td>0.0084</td>\n",
       "      <td>0.0054</td>\n",
       "      <td>0.1141</td>\n",
       "      <td>0.0216</td>\n",
       "      <td>0.0034</td>\n",
       "      <td>0.1116</td>\n",
       "      <td>0.0329</td>\n",
       "      <td>0.0265</td>\n",
       "      <td>0.0919</td>\n",
       "      <td>0.0108</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0088</td>\n",
       "      <td>0.0133</td>\n",
       "      <td>0.0029</td>\n",
       "      <td>0.0064</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.1131</td>\n",
       "      <td>0.3255</td>\n",
       "      <td>2034</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              tweets  label  \\\n",
       "0  [{'tweet_content': '请多多注意防护预防感染常戴口罩', 'posting...      0   \n",
       "1  [{'tweet_content': '你敢不敢让我中个小恐龙', 'posting_tim...      0   \n",
       "2  [{'tweet_content': '滴 613打卡成功 天气好的一天 看见小泽自拍 心情...      0   \n",
       "3  [{'tweet_content': '有没有人跟我一样的卡了我2天52个精英令用都用不鸟这...      0   \n",
       "4  [{'tweet_content': '婚姻不分年龄希望决定结婚之前能够想清楚对方是你想要相...      0   \n",
       "\n",
       "                                         tweets_flat  \\\n",
       "0  请多多注意防护预防感染常戴口罩 走在乡间的小路上乡村风景美如画 空运的螃蟹到了快来领取啊 何...   \n",
       "1  你敢不敢让我中个小恐龙 周年庆邀你盛装出席 5月1日5月8日参与周年庆活动分享你的周年庆时装...   \n",
       "2  滴 613打卡成功 天气好的一天 看见小泽自拍 心情更加好 以后多发呗 早上好呀天泽 天气好...   \n",
       "3  有没有人跟我一样的卡了我2天52个精英令用都用不鸟这波过去不卸载我是 我在打卡啦每日签到领红...   \n",
       "4  婚姻不分年龄希望决定结婚之前能够想清楚对方是你想要相伴一生的人而且不管前路好坏面对诱惑时你还...   \n",
       "\n",
       "                                      tweets_chinese  \\\n",
       "0  请多多注意防护预防感染常戴口罩 走在乡间的小路上乡村风景美如画 空运的螃蟹到了快来领取啊 何...   \n",
       "1  你敢不敢让我中个小恐龙 周年庆邀你盛装出席  月 日 月 日参与周年庆活动分享你的周年庆时装...   \n",
       "2  滴    打卡成功 天气好的一天 看见小泽自拍 心情更加好 以后多发呗 早上好呀天泽 天气好...   \n",
       "3  有没有人跟我一样的卡了我 天  个精英令用都用不鸟这波过去不卸载我是 我在打卡啦每日签到领红...   \n",
       "4  婚姻不分年龄希望决定结婚之前能够想清楚对方是你想要相伴一生的人而且不管前路好坏面对诱惑时你还...   \n",
       "\n",
       "                                          tweets_cut  \\\n",
       "0  请 多多 注意 防护 预防 感染 常 戴口罩   走 在 乡间 的 小路 上 乡村 风景 美...   \n",
       "1  你 敢不敢 让 我 中 个 小 恐龙   周年庆 邀 你 盛装 出席   月 日 月 日 参...   \n",
       "2  滴     打卡 成功   天气 好 的 一天   看见 小泽 自拍   心情 更加 好  ...   \n",
       "3  有没有 人 跟 我 一样 的 卡 了 我  天   个 精英令 用 都 用 不 鸟 这 波 ...   \n",
       "4  婚姻 不分 年龄 希望 决定 结婚之前 能够 想清楚 对方 是 你 想要 相伴 一生 的 人...   \n",
       "\n",
       "                                        tweets_clean      n      f      s  \\\n",
       "0  多多 注意 防护 预防 感染 常 戴口罩   走 乡间 小路 乡村 风景 美如画 空运 螃蟹... 0.1953 0.0163 0.0063   \n",
       "1  敢不敢 恐龙   周年庆 邀 盛装 出席   参与 周年庆 活动 分享 周年庆 时装 穿 搭... 0.1860 0.0123 0.0036   \n",
       "2  滴     打卡 成功   天气 一天   看见 小泽 自拍   心情 更加  以后 多发 ... 0.2109 0.0339 0.0000   \n",
       "3  有没有 卡  天   精英令 鸟 波 过去 卸载   打卡 每日 签到 领 红包 签到 越多... 0.2083 0.0043 0.0064   \n",
       "4  婚姻 不分 年龄 希望 决定 结婚之前 能够 想清楚 想要 相伴 一生 前路 好坏 面对 诱... 0.1676 0.0113 0.0020   \n",
       "\n",
       "       t     nr     ns     nt     nw     nz      v     vd     vn      a  \\\n",
       "0 0.0056 0.0163 0.0010 0.0031 0.0003 0.0167 0.2651 0.0021 0.0233 0.0584   \n",
       "1 0.0036 0.0342 0.0000 0.0015 0.0000 0.0061 0.3296 0.0005 0.0092 0.0644   \n",
       "2 0.0000 0.0417 0.0000 0.0000 0.0000 0.0026 0.3021 0.0026 0.0026 0.0990   \n",
       "3 0.0036 0.0228 0.0000 0.0021 0.0000 0.0114 0.3210 0.0014 0.0164 0.0599   \n",
       "4 0.0088 0.0113 0.0000 0.0005 0.0000 0.0034 0.3107 0.0015 0.0182 0.0610   \n",
       "\n",
       "      ad     an      d      m      q      r      p      c      u     xc  \\\n",
       "0 0.0049 0.0031 0.0702 0.0274 0.0045 0.1028 0.0434 0.0285 0.1171 0.0125   \n",
       "1 0.0072 0.0015 0.0991 0.0225 0.0107 0.1139 0.0255 0.0184 0.0894 0.0388   \n",
       "2 0.0052 0.0000 0.0573 0.0052 0.0026 0.0286 0.0078 0.0078 0.0807 0.0729   \n",
       "3 0.0064 0.0093 0.0792 0.0250 0.0064 0.1013 0.0371 0.0136 0.1020 0.0371   \n",
       "4 0.0084 0.0054 0.1141 0.0216 0.0034 0.1116 0.0329 0.0265 0.0919 0.0108   \n",
       "\n",
       "       w    PER    LOC    ORG   TIME      O  n_pos  n_neg  word_count  \n",
       "0 0.0007 0.0083 0.0379 0.0045 0.0049 0.0000 0.1147 0.3805        2878  \n",
       "1 0.0015 0.0036 0.0041 0.0005 0.0210 0.0000 0.0772 0.3587        1957  \n",
       "2 0.0000 0.0339 0.0000 0.0000 0.0729 0.0000 0.1406 0.3203         384  \n",
       "3 0.0014 0.0064 0.0014 0.0036 0.0071 0.0000 0.1134 0.3495        1402  \n",
       "4 0.0000 0.0088 0.0133 0.0029 0.0064 0.0000 0.1131 0.3255        2034  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fe_cols = pos_cols+['n_pos', 'n_neg']\n",
    "\n",
    "df_train['word_count'] = df_train.iloc[:, -31:-2].sum(axis=1)\n",
    "for col in fe_cols:\n",
    "    df_train[col] = df_train[col]/df_train['word_count']\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.to_csv('df_train.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(21073, 115217)\n",
      "(21073,)\n"
     ]
    }
   ],
   "source": [
    "count_vect = CountVectorizer(binary=False, max_df=0.6, min_df=5)\n",
    "#                             token_pattern='[a-z]{2,15}', \n",
    "                            \n",
    "X = count_vect.fit_transform(df_train['tweets_clean'].astype('str')) # text features\n",
    "y = df_train['label'].values # target\n",
    "\n",
    "X_fe = df_train[fe_cols].to_numpy()\n",
    "X = hstack([X, X_fe])\n",
    "\n",
    "print(X.shape)\n",
    "print(y.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.91      0.92      3224\n",
      "           1       0.91      0.93      0.92      3098\n",
      "\n",
      "    accuracy                           0.92      6322\n",
      "   macro avg       0.92      0.92      0.92      6322\n",
      "weighted avg       0.92      0.92      0.92      6322\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, random_state = 1, test_size=0.3)\n",
    "\n",
    "nb = MultinomialNB()\n",
    "nb.fit(X_train, y_train)\n",
    "nb_pred = nb.predict(X_test)\n",
    "\n",
    "print(classification_report(y_test, nb_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['视频' '一起' '中国' '红包' '努力' '人生' '快手' '工作' '孩子' '分享']\n",
      "['痛苦' '抑郁症' '抑郁' '情绪' '孩子' '不知道' '一个人' '事情' '很多' '其实']\n"
     ]
    }
   ],
   "source": [
    "# feature importance for Navie Bayes\n",
    "normal_prob_sorted = nb.feature_log_prob_[0, :].argsort()[::-1]\n",
    "depressed_prob_sorted = nb.feature_log_prob_[1, :].argsort()[::-1]\n",
    "\n",
    "print(np.take(count_vect.get_feature_names()+fe_cols, normal_prob_sorted[:10]))\n",
    "print(np.take(count_vect.get_feature_names()+fe_cols, depressed_prob_sorted[:10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TFIDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(21073, 70853)\n",
      "(21073,)\n"
     ]
    }
   ],
   "source": [
    "tfidf_vect = TfidfVectorizer(ngram_range=(1, 1), min_df=10)\n",
    "\n",
    "X = tfidf_vect.fit_transform(df_train['tweets_clean'].astype('str')) # text features\n",
    "y = df_train['label'].values # target\n",
    "\n",
    "X_fe = df_train[fe_cols].to_numpy()\n",
    "X = hstack([X, X_fe])\n",
    "\n",
    "print (X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16858, 70853)\n",
      "(16858,)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model_name</th>\n",
       "      <th>accuracy_score</th>\n",
       "      <th>precision_score</th>\n",
       "      <th>recall_score</th>\n",
       "      <th>f1_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>XGBoost</td>\n",
       "      <td>0.9580</td>\n",
       "      <td>0.9587</td>\n",
       "      <td>0.9577</td>\n",
       "      <td>0.9580</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AdaBoost</td>\n",
       "      <td>0.9544</td>\n",
       "      <td>0.9549</td>\n",
       "      <td>0.9542</td>\n",
       "      <td>0.9544</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.9521</td>\n",
       "      <td>0.9539</td>\n",
       "      <td>0.9515</td>\n",
       "      <td>0.9520</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Stochastic Gradient Descent</td>\n",
       "      <td>0.9499</td>\n",
       "      <td>0.9518</td>\n",
       "      <td>0.9494</td>\n",
       "      <td>0.9498</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>0.9473</td>\n",
       "      <td>0.9486</td>\n",
       "      <td>0.9469</td>\n",
       "      <td>0.9472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Decsision Tree</td>\n",
       "      <td>0.9345</td>\n",
       "      <td>0.9345</td>\n",
       "      <td>0.9345</td>\n",
       "      <td>0.9345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>K Nearest Neighbor</td>\n",
       "      <td>0.8925</td>\n",
       "      <td>0.8925</td>\n",
       "      <td>0.8924</td>\n",
       "      <td>0.8925</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    model_name accuracy_score precision_score recall_score  \\\n",
       "5                      XGBoost         0.9580          0.9587       0.9577   \n",
       "4                     AdaBoost         0.9544          0.9549       0.9542   \n",
       "0          Logistic Regression         0.9521          0.9539       0.9515   \n",
       "1  Stochastic Gradient Descent         0.9499          0.9518       0.9494   \n",
       "2                Random Forest         0.9473          0.9486       0.9469   \n",
       "3               Decsision Tree         0.9345          0.9345       0.9345   \n",
       "6           K Nearest Neighbor         0.8925          0.8925       0.8924   \n",
       "\n",
       "  f1_score  \n",
       "5   0.9580  \n",
       "4   0.9544  \n",
       "0   0.9520  \n",
       "1   0.9498  \n",
       "2   0.9472  \n",
       "3   0.9345  \n",
       "6   0.8925  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "#Preliminary model evaluation using default parameters\n",
    "\n",
    "#Creating a dict of the models\n",
    "model_dict = {'Logistic Regression': LogisticRegression(random_state=3),\n",
    "              'Stochastic Gradient Descent' : SGDClassifier(random_state=3, loss='log'),\n",
    "              'Random Forest': RandomForestClassifier(random_state=3),\n",
    "              'Decsision Tree': DecisionTreeClassifier(random_state=3),\n",
    "              'AdaBoost': AdaBoostClassifier(random_state=3),\n",
    "              'XGBoost': XGBClassifier(random_state=3),\n",
    "              'K Nearest Neighbor': KNeighborsClassifier()}\n",
    "\n",
    "#Train test split with stratified sampling for evaluation\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, \n",
    "                                                    y, \n",
    "                                                    test_size = .2, \n",
    "                                                    shuffle = True, \n",
    "                                                    stratify = y, \n",
    "                                                    random_state = 3)\n",
    "\n",
    "print (X_train.shape)\n",
    "print(y_train.shape)\n",
    "\n",
    "#Function to get the scores for each model in a df\n",
    "def model_score_df(model_dict):   \n",
    "    model_name, ac_score_list, p_score_list, r_score_list, f1_score_list = [], [], [], [], []\n",
    "    for k,v in model_dict.items():   \n",
    "        model_name.append(k)\n",
    "        v.fit(X_train, y_train)\n",
    "        y_pred = v.predict(X_test)\n",
    "        ac_score_list.append(accuracy_score(y_test, y_pred))\n",
    "        p_score_list.append(precision_score(y_test, y_pred, average='macro'))\n",
    "        r_score_list.append(recall_score(y_test, y_pred, average='macro'))\n",
    "        f1_score_list.append(f1_score(y_test, y_pred, average='macro'))\n",
    "        model_comparison_df = pd.DataFrame([model_name, ac_score_list, p_score_list, r_score_list, f1_score_list]).T\n",
    "        model_comparison_df.columns = ['model_name', 'accuracy_score', 'precision_score', 'recall_score', 'f1_score']\n",
    "        model_comparison_df = model_comparison_df.sort_values(by='f1_score', ascending=False)\n",
    "    return model_comparison_df\n",
    "\n",
    "model_score_df(model_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9615627965833597\n",
      "0.9619342656020473\n",
      "0.9613362178752962\n",
      "0.9615269405404674\n"
     ]
    }
   ],
   "source": [
    "# X_train, X_test, y_train, y_test = train_test_split(X, \n",
    "#                                                     y, \n",
    "#                                                     test_size = .3, \n",
    "#                                                     shuffle = True, \n",
    "#                                                     stratify = y, \n",
    "#                                                     random_state = 3)\n",
    "\n",
    "v = XGBClassifier(learning_rate=0.5, n_estimater=75, max_depth=3, objective='binary:logistic', random_state=3)\n",
    "# v = XGBClassifier(objective='binary:logistic', random_state=3)\n",
    "\n",
    "\n",
    "# v = AdaBoostClassifier(n_estimators=500, random_state=3)\n",
    "v.fit(X_train, y_train)\n",
    "y_pred = v.predict(X_test)\n",
    "print(accuracy_score(y_test, y_pred))\n",
    "print(precision_score(y_test, y_pred, average='macro'))\n",
    "print(recall_score(y_test, y_pred, average='macro'))\n",
    "print(f1_score(y_test, y_pred, average='macro'))\n",
    "\n",
    "# 0.9647263524201202\n",
    "# 0.9650854385082227\n",
    "# 0.9645073384413542\n",
    "# 0.964694126545599"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter Tuning\n",
    "- XGBoost\n",
    "- AdaBoost\n",
    "- Logistic Regression\t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hyperparameter tuning\n",
    "#Gridsearch with 5-fold cross validation\n",
    "\n",
    "#LR\n",
    "solver =  ['saga', 'liblinear', 'lbfgs']\n",
    "penalty = ['l2','l1']\n",
    "C = [100, 10, 1.0, 0.1]\n",
    "max_iter = [100, 500]\n",
    "random_state = [3]\n",
    "\n",
    "lr = LogisticRegression()\n",
    "\n",
    "params = dict(solver=solver,\n",
    "              penalty=penalty,\n",
    "              C=C,\n",
    "              max_iter=max_iter,\n",
    "              random_state=random_state)\n",
    "\n",
    "gridsearch = GridSearchCV(lr,\n",
    "                          params,\n",
    "                          cv = 5,\n",
    "                          scoring = 'recall',\n",
    "                          verbose = 1, \n",
    "                          n_jobs = -1)\n",
    "\n",
    "lr_best_model = gridsearch.fit(X, y)\n",
    "# LogisticRegression(solver='liblinear', C=0.1, penalty='l2', random_state=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hyperparameter tuning\n",
    "#Gridsearch with 5-fold cross validation\n",
    "#Warning this can take a long time!!!\n",
    "\n",
    "#SGD\n",
    "loss =  ['log']\n",
    "penalty = ['l2','l1']\n",
    "alpha = [1e-6, 1e-3, 1e-1, 1e0]\n",
    "max_iter = [5, 1000, 10000]\n",
    "tol = [None, 1e-3]\n",
    "eta0 = [0.1, 0.001]\n",
    "\n",
    "random_state = [3]\n",
    "\n",
    "clf = SGDClassifier()\n",
    "\n",
    "params = dict(loss=loss,\n",
    "              penalty=penalty,\n",
    "              alpha=alpha,\n",
    "              max_iter=max_iter,\n",
    "              tol=tol,\n",
    "              random_state=random_state)\n",
    "\n",
    "gridsearch = GridSearchCV(clf,\n",
    "                          params,\n",
    "                          cv = 5,\n",
    "                          scoring = 'recall',\n",
    "                          verbose = 1, \n",
    "                          n_jobs = -1)\n",
    "\n",
    "sgd_best_model = gridsearch.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 20 candidates, totalling 100 fits\n"
     ]
    }
   ],
   "source": [
    "#Hyperparameter tuning\n",
    "#Gridsearch with 5-fold cross validation\n",
    "#Warning this can take a long time!!!\n",
    "\n",
    "#AdaBoost\n",
    "n_estimators = [10, 50, 100, 500]\n",
    "learning_rate = [0.0001, 0.001, 0.01, 0.1, 1.0]\n",
    "\n",
    "random_state = [3]\n",
    "\n",
    "clf = AdaBoostClassifier()\n",
    "\n",
    "params = dict(n_estimators=n_estimators,\n",
    "              learning_rate=learning_rate,\n",
    "              random_state=random_state)\n",
    "\n",
    "gridsearch = GridSearchCV(clf,\n",
    "                          params,\n",
    "                          cv = 5,\n",
    "                          scoring = 'recall',\n",
    "                          verbose = 1, \n",
    "                          n_jobs = -1)\n",
    "\n",
    "ada_best_model = gridsearch.fit(X, y)\n",
    "# AdaBoostClassifier(n_estimators=500, random_state=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hyperparameter tuning\n",
    "#Gridsearch with 5-fold cross validation\n",
    "\n",
    "#Xgbost\n",
    "n_estimators = [50,75,100]\n",
    "max_depth = [3,6,9]\n",
    "learning_rate = [0.01,0.1,0.5]\n",
    "random_state = [3]\n",
    "\n",
    "clf = XGBClassifier(objective='binary:logistic',eval_metric='error')\n",
    "\n",
    "params = dict(n_estimators=n_estimators,\n",
    "              max_depth = max_depth,\n",
    "              learning_rate = learning_rate,\n",
    "              random_state=random_state)\n",
    "\n",
    "gridsearch = GridSearchCV(clf,\n",
    "                          params,\n",
    "                          cv = 5,\n",
    "                          scoring = 'recall',\n",
    "                          verbose = 1,\n",
    "                          n_jobs = -1)\n",
    "\n",
    "xgb_best_model = gridsearch.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_best_model.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AdaBoostClassifier(n_estimators=500, random_state=3)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ada_best_model.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_best_model.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test On New Dataset Test1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_new_depressed = pd.read_csv('test_depressed_new.csv', index_col=0)\n",
    "test_new_depressed['label'] = 1\n",
    "test_new_normal = pd.read_csv('test_normal_new.csv', index_col=0)\n",
    "test_new_normal['label'] = 0\n",
    "# test_new = pd.concat([test_new_depressed, test_new_normal], axis=0)\n",
    "# test_new.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweets</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>姐姐妹妹  奇迹暖暖 真的没人觉得这个摆件像鞋垫吗？还是卷了的，超级不舒服的感觉  抑郁症抑...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>总之不要放弃就对了吧？  今天的花好好看  真的太敏感太脆弱了。再也受不了任何委屈了。</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>张国荣#十七周年继续宠爱张国荣# 茫然兩眼已漸紅 皆因想你  想他嗎？  #男朋友是怎么宠你...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>手抖冒冷汗心跳快一下午了  真好 下雨又不用出门了  第五人格出个号子 网页链接成功出的话从...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>我吃果子只是为了跟花有点联系——《顾城诗集》</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              tweets  label\n",
       "0  姐姐妹妹  奇迹暖暖 真的没人觉得这个摆件像鞋垫吗？还是卷了的，超级不舒服的感觉  抑郁症抑...      1\n",
       "1       总之不要放弃就对了吧？  今天的花好好看  真的太敏感太脆弱了。再也受不了任何委屈了。       1\n",
       "2  张国荣#十七周年继续宠爱张国荣# 茫然兩眼已漸紅 皆因想你  想他嗎？  #男朋友是怎么宠你...      1\n",
       "3  手抖冒冷汗心跳快一下午了  真好 下雨又不用出门了  第五人格出个号子 网页链接成功出的话从...      1\n",
       "4                            我吃果子只是为了跟花有点联系——《顾城诗集》       1"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_new_depressed.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweets</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>又染得比黑更黑  害 好想吃烤肉啊(指么子烤肉)  嗯嗯真好  //  On April 2...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>今天的菜有点优秀  阿迪耐克彪马不能买了 鬼塚虎没问题吧 这些牌子一作死 没鞋穿了  这次搬...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>#集福牛开福运# 福运正当头，牛年我最牛！我在福运红包中开出了 @让红包飞 送出的【2.18...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>#中国超2亿人单身#哈哈哈哈哈哈没有我我已经有金珉锡了哈哈哈哈😄  我在#森林驿站#帮助了大...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>每日一善在微博多发大牌的名字 他会检测到你消费偏好比较高 然后提升你的信用分中奖率会提高我希...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              tweets  label\n",
       "0  又染得比黑更黑  害 好想吃烤肉啊(指么子烤肉)  嗯嗯真好  //  On April 2...      0\n",
       "1  今天的菜有点优秀  阿迪耐克彪马不能买了 鬼塚虎没问题吧 这些牌子一作死 没鞋穿了  这次搬...      0\n",
       "2  #集福牛开福运# 福运正当头，牛年我最牛！我在福运红包中开出了 @让红包飞 送出的【2.18...      0\n",
       "3  #中国超2亿人单身#哈哈哈哈哈哈没有我我已经有金珉锡了哈哈哈哈😄  我在#森林驿站#帮助了大...      0\n",
       "4  每日一善在微博多发大牌的名字 他会检测到你消费偏好比较高 然后提升你的信用分中奖率会提高我希...      0"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_new_normal.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweets</th>\n",
       "      <th>label</th>\n",
       "      <th>tweets_chinese</th>\n",
       "      <th>tweets_cut</th>\n",
       "      <th>tweets_clean</th>\n",
       "      <th>n</th>\n",
       "      <th>f</th>\n",
       "      <th>s</th>\n",
       "      <th>t</th>\n",
       "      <th>nr</th>\n",
       "      <th>ns</th>\n",
       "      <th>nt</th>\n",
       "      <th>nw</th>\n",
       "      <th>nz</th>\n",
       "      <th>v</th>\n",
       "      <th>vd</th>\n",
       "      <th>vn</th>\n",
       "      <th>a</th>\n",
       "      <th>ad</th>\n",
       "      <th>an</th>\n",
       "      <th>d</th>\n",
       "      <th>m</th>\n",
       "      <th>q</th>\n",
       "      <th>r</th>\n",
       "      <th>p</th>\n",
       "      <th>c</th>\n",
       "      <th>u</th>\n",
       "      <th>xc</th>\n",
       "      <th>w</th>\n",
       "      <th>PER</th>\n",
       "      <th>LOC</th>\n",
       "      <th>ORG</th>\n",
       "      <th>TIME</th>\n",
       "      <th>O</th>\n",
       "      <th>n_pos</th>\n",
       "      <th>n_neg</th>\n",
       "      <th>word_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>姐姐妹妹  奇迹暖暖 真的没人觉得这个摆件像鞋垫吗？还是卷了的，超级不舒服的感觉  抑郁症抑...</td>\n",
       "      <td>1</td>\n",
       "      <td>姐姐妹妹  奇迹暖暖 真的没人觉得这个摆件像鞋垫吗 还是卷了的 超级不舒服的感觉  抑郁症抑...</td>\n",
       "      <td>姐姐 妹妹    奇迹 暖暖  真 的 没 人 觉得 这个 摆件 像 鞋垫 吗   还是 卷...</td>\n",
       "      <td>姐姐 妹妹    奇迹 暖暖  真 没 觉得 摆件 鞋垫   卷   超级 不舒服 感觉  ...</td>\n",
       "      <td>0.1571</td>\n",
       "      <td>0.0130</td>\n",
       "      <td>0.0031</td>\n",
       "      <td>0.0061</td>\n",
       "      <td>0.0282</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0160</td>\n",
       "      <td>0.3204</td>\n",
       "      <td>0.0038</td>\n",
       "      <td>0.0061</td>\n",
       "      <td>0.0412</td>\n",
       "      <td>0.0084</td>\n",
       "      <td>0.0031</td>\n",
       "      <td>0.0694</td>\n",
       "      <td>0.0214</td>\n",
       "      <td>0.0031</td>\n",
       "      <td>0.1320</td>\n",
       "      <td>0.0252</td>\n",
       "      <td>0.0229</td>\n",
       "      <td>0.0854</td>\n",
       "      <td>0.0137</td>\n",
       "      <td>0.0008</td>\n",
       "      <td>0.0031</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0008</td>\n",
       "      <td>0.0160</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0450</td>\n",
       "      <td>0.3547</td>\n",
       "      <td>1311</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>总之不要放弃就对了吧？  今天的花好好看  真的太敏感太脆弱了。再也受不了任何委屈了。</td>\n",
       "      <td>1</td>\n",
       "      <td>总之不要放弃就对了吧   今天的花好好看  真的太敏感太脆弱了 再也受不了任何委屈了</td>\n",
       "      <td>总之 不要 放弃 就 对 了 吧     今天 的 花 好 好看  真 的 太 敏感 太 脆...</td>\n",
       "      <td>不要 放弃     今天 花 好看  真 太 敏感 太 脆弱   受不了 委屈</td>\n",
       "      <td>0.0769</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0385</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.1154</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0385</td>\n",
       "      <td>0.1154</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.2308</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0385</td>\n",
       "      <td>0.0385</td>\n",
       "      <td>0.0385</td>\n",
       "      <td>0.1154</td>\n",
       "      <td>0.1154</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0385</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.1154</td>\n",
       "      <td>0.3077</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>张国荣#十七周年继续宠爱张国荣# 茫然兩眼已漸紅 皆因想你  想他嗎？  #男朋友是怎么宠你...</td>\n",
       "      <td>1</td>\n",
       "      <td>张国荣 十七周年继续宠爱张国荣  茫然兩眼已漸紅 皆因想你  想他嗎    男朋友是怎么宠你...</td>\n",
       "      <td>张国荣   十七周年 继续 宠爱 张国荣    茫然 兩眼 已 漸 紅  皆因 想你    ...</td>\n",
       "      <td>张国荣   十七周年 继续 宠爱 张国荣    茫然 兩眼 漸 紅  皆因 想你    想 ...</td>\n",
       "      <td>0.0956</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.1140</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0110</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0257</td>\n",
       "      <td>0.2868</td>\n",
       "      <td>0.0257</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0478</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0404</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.1434</td>\n",
       "      <td>0.0110</td>\n",
       "      <td>0.0037</td>\n",
       "      <td>0.0882</td>\n",
       "      <td>0.0294</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0368</td>\n",
       "      <td>0.0037</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0368</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0882</td>\n",
       "      <td>0.2684</td>\n",
       "      <td>272</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>手抖冒冷汗心跳快一下午了  真好 下雨又不用出门了  第五人格出个号子 网页链接成功出的话从...</td>\n",
       "      <td>1</td>\n",
       "      <td>手抖冒冷汗心跳快一下午了  真好 下雨又不用出门了  第五人格出个号子 网页链接成功出的话从...</td>\n",
       "      <td>手抖 冒 冷汗 心跳 快 一 下午 了    真好  下雨 又 不用 出门 了    第五 ...</td>\n",
       "      <td>手抖 冷汗 心跳 快 下午    真好  下雨 不用 出门    第五 人格 出 号子   ...</td>\n",
       "      <td>0.1650</td>\n",
       "      <td>0.0061</td>\n",
       "      <td>0.0017</td>\n",
       "      <td>0.0050</td>\n",
       "      <td>0.0742</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0050</td>\n",
       "      <td>0.0011</td>\n",
       "      <td>0.0205</td>\n",
       "      <td>0.2935</td>\n",
       "      <td>0.0006</td>\n",
       "      <td>0.0094</td>\n",
       "      <td>0.0476</td>\n",
       "      <td>0.0078</td>\n",
       "      <td>0.0022</td>\n",
       "      <td>0.0714</td>\n",
       "      <td>0.0371</td>\n",
       "      <td>0.0083</td>\n",
       "      <td>0.0753</td>\n",
       "      <td>0.0194</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>0.0764</td>\n",
       "      <td>0.0305</td>\n",
       "      <td>0.0006</td>\n",
       "      <td>0.0144</td>\n",
       "      <td>0.0006</td>\n",
       "      <td>0.0011</td>\n",
       "      <td>0.0155</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0952</td>\n",
       "      <td>0.3295</td>\n",
       "      <td>1806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>我吃果子只是为了跟花有点联系——《顾城诗集》</td>\n",
       "      <td>1</td>\n",
       "      <td>我吃果子只是为了跟花有点联系   顾城诗集</td>\n",
       "      <td>我 吃 果子 只是 为了 跟 花 有点 联系     顾城 诗集</td>\n",
       "      <td>吃 果子 花 有点 联系     顾城 诗集</td>\n",
       "      <td>0.2500</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0833</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.1667</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.1667</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0833</td>\n",
       "      <td>0.1667</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0833</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.3333</td>\n",
       "      <td>0.2500</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              tweets  label  \\\n",
       "0  姐姐妹妹  奇迹暖暖 真的没人觉得这个摆件像鞋垫吗？还是卷了的，超级不舒服的感觉  抑郁症抑...      1   \n",
       "1       总之不要放弃就对了吧？  今天的花好好看  真的太敏感太脆弱了。再也受不了任何委屈了。       1   \n",
       "2  张国荣#十七周年继续宠爱张国荣# 茫然兩眼已漸紅 皆因想你  想他嗎？  #男朋友是怎么宠你...      1   \n",
       "3  手抖冒冷汗心跳快一下午了  真好 下雨又不用出门了  第五人格出个号子 网页链接成功出的话从...      1   \n",
       "4                            我吃果子只是为了跟花有点联系——《顾城诗集》       1   \n",
       "\n",
       "                                      tweets_chinese  \\\n",
       "0  姐姐妹妹  奇迹暖暖 真的没人觉得这个摆件像鞋垫吗 还是卷了的 超级不舒服的感觉  抑郁症抑...   \n",
       "1       总之不要放弃就对了吧   今天的花好好看  真的太敏感太脆弱了 再也受不了任何委屈了     \n",
       "2  张国荣 十七周年继续宠爱张国荣  茫然兩眼已漸紅 皆因想你  想他嗎    男朋友是怎么宠你...   \n",
       "3  手抖冒冷汗心跳快一下午了  真好 下雨又不用出门了  第五人格出个号子 网页链接成功出的话从...   \n",
       "4                            我吃果子只是为了跟花有点联系   顾城诗集     \n",
       "\n",
       "                                          tweets_cut  \\\n",
       "0  姐姐 妹妹    奇迹 暖暖  真 的 没 人 觉得 这个 摆件 像 鞋垫 吗   还是 卷...   \n",
       "1  总之 不要 放弃 就 对 了 吧     今天 的 花 好 好看  真 的 太 敏感 太 脆...   \n",
       "2  张国荣   十七周年 继续 宠爱 张国荣    茫然 兩眼 已 漸 紅  皆因 想你    ...   \n",
       "3  手抖 冒 冷汗 心跳 快 一 下午 了    真好  下雨 又 不用 出门 了    第五 ...   \n",
       "4                   我 吃 果子 只是 为了 跟 花 有点 联系     顾城 诗集   \n",
       "\n",
       "                                        tweets_clean      n      f      s  \\\n",
       "0  姐姐 妹妹    奇迹 暖暖  真 没 觉得 摆件 鞋垫   卷   超级 不舒服 感觉  ... 0.1571 0.0130 0.0031   \n",
       "1            不要 放弃     今天 花 好看  真 太 敏感 太 脆弱   受不了 委屈 0.0769 0.0000 0.0000   \n",
       "2  张国荣   十七周年 继续 宠爱 张国荣    茫然 兩眼 漸 紅  皆因 想你    想 ... 0.0956 0.0000 0.0000   \n",
       "3  手抖 冷汗 心跳 快 下午    真好  下雨 不用 出门    第五 人格 出 号子   ... 0.1650 0.0061 0.0017   \n",
       "4                             吃 果子 花 有点 联系     顾城 诗集 0.2500 0.0000 0.0000   \n",
       "\n",
       "       t     nr     ns     nt     nw     nz      v     vd     vn      a  \\\n",
       "0 0.0061 0.0282 0.0000 0.0000 0.0000 0.0160 0.3204 0.0038 0.0061 0.0412   \n",
       "1 0.0000 0.0385 0.0000 0.0000 0.0000 0.0000 0.1154 0.0000 0.0385 0.1154   \n",
       "2 0.0000 0.1140 0.0000 0.0110 0.0000 0.0257 0.2868 0.0257 0.0000 0.0478   \n",
       "3 0.0050 0.0742 0.0000 0.0050 0.0011 0.0205 0.2935 0.0006 0.0094 0.0476   \n",
       "4 0.0000 0.0833 0.0000 0.0000 0.0000 0.0000 0.1667 0.0000 0.0000 0.0000   \n",
       "\n",
       "      ad     an      d      m      q      r      p      c      u     xc  \\\n",
       "0 0.0084 0.0031 0.0694 0.0214 0.0031 0.1320 0.0252 0.0229 0.0854 0.0137   \n",
       "1 0.0000 0.0000 0.2308 0.0000 0.0000 0.0385 0.0385 0.0385 0.1154 0.1154   \n",
       "2 0.0000 0.0000 0.0404 0.0000 0.0000 0.1434 0.0110 0.0037 0.0882 0.0294   \n",
       "3 0.0078 0.0022 0.0714 0.0371 0.0083 0.0753 0.0194 0.0100 0.0764 0.0305   \n",
       "4 0.0000 0.0000 0.1667 0.0000 0.0000 0.0833 0.1667 0.0000 0.0000 0.0000   \n",
       "\n",
       "       w    PER    LOC    ORG   TIME      O  n_pos  n_neg  word_count  \n",
       "0 0.0008 0.0031 0.0000 0.0008 0.0160 0.0000 0.0450 0.3547        1311  \n",
       "1 0.0000 0.0000 0.0000 0.0000 0.0385 0.0000 0.1154 0.3077          26  \n",
       "2 0.0000 0.0368 0.0037 0.0000 0.0368 0.0000 0.0882 0.2684         272  \n",
       "3 0.0006 0.0144 0.0006 0.0011 0.0155 0.0000 0.0952 0.3295        1806  \n",
       "4 0.0000 0.0833 0.0000 0.0000 0.0000 0.0000 0.3333 0.2500          12  "
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# preprocess text on test data\n",
    "\n",
    "test_new_depressed['tweets_chinese'] = test_new_depressed['tweets'].apply(lambda tweets: format_str(tweets))\n",
    "  \n",
    "test_new_depressed['tweets_cut'] = test_new_depressed['tweets_chinese'].apply(lambda t: ' '.join(list(jieba.cut(t, \n",
    "                                                                                 use_paddle=True\n",
    "#                                                                                  cut_all=True\n",
    "                                                                                ))))\n",
    "test_new_depressed['tweets_clean'] = test_new_depressed['tweets_cut'].apply(lambda t: ' '.join([word for word in t.split(' ') if word not in STOPWORDS ]))\n",
    "\n",
    "test_new_depressed = generate_pos(test_new_depressed)\n",
    "test_new_depressed = generate_sentiment(test_new_depressed)\n",
    "\n",
    "\n",
    "test_new_normal['tweets_chinese'] = test_new_normal['tweets'].apply(lambda tweets: format_str(tweets))\n",
    "  \n",
    "test_new_normal['tweets_cut'] = test_new_normal['tweets_chinese'].apply(lambda t: ' '.join(list(jieba.cut(t, \n",
    "                                                                                 use_paddle=True\n",
    "#                                                                                  cut_all=True\n",
    "                                                                                ))))\n",
    "test_new_normal['tweets_clean'] = test_new_normal['tweets_cut'].apply(lambda t: ' '.join([word for word in t.split(' ') if word not in STOPWORDS ]))\n",
    "\n",
    "\n",
    "test_new_normal = generate_pos(test_new_normal)\n",
    "test_new_normal = generate_sentiment(test_new_normal)\n",
    "\n",
    "test_new = pd.concat([test_new_depressed, test_new_normal], axis=0)\n",
    "\n",
    "test_new['word_count'] = test_new.iloc[:, -31:-2].sum(axis=1)\n",
    "for col in fe_cols:\n",
    "    test_new[col] = test_new[col]/test_new['word_count']\n",
    "\n",
    "test_new.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(400, 37)"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_new.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model_name</th>\n",
       "      <th>accuracy_score</th>\n",
       "      <th>precision_score</th>\n",
       "      <th>recall_score</th>\n",
       "      <th>f1_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>XGBoost</td>\n",
       "      <td>0.8400</td>\n",
       "      <td>0.8422</td>\n",
       "      <td>0.8400</td>\n",
       "      <td>0.8397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Decsision Tree</td>\n",
       "      <td>0.8300</td>\n",
       "      <td>0.8301</td>\n",
       "      <td>0.8300</td>\n",
       "      <td>0.8300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AdaBoost</td>\n",
       "      <td>0.8175</td>\n",
       "      <td>0.8179</td>\n",
       "      <td>0.8175</td>\n",
       "      <td>0.8174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.7975</td>\n",
       "      <td>0.8022</td>\n",
       "      <td>0.7975</td>\n",
       "      <td>0.7967</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Stochastic Gradient Descent</td>\n",
       "      <td>0.7825</td>\n",
       "      <td>0.7863</td>\n",
       "      <td>0.7825</td>\n",
       "      <td>0.7818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>0.7625</td>\n",
       "      <td>0.7793</td>\n",
       "      <td>0.7625</td>\n",
       "      <td>0.7589</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>K Nearest Neighbor</td>\n",
       "      <td>0.6950</td>\n",
       "      <td>0.6989</td>\n",
       "      <td>0.6950</td>\n",
       "      <td>0.6935</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    model_name accuracy_score precision_score recall_score  \\\n",
       "5                      XGBoost         0.8400          0.8422       0.8400   \n",
       "3               Decsision Tree         0.8300          0.8301       0.8300   \n",
       "4                     AdaBoost         0.8175          0.8179       0.8175   \n",
       "0          Logistic Regression         0.7975          0.8022       0.7975   \n",
       "1  Stochastic Gradient Descent         0.7825          0.7863       0.7825   \n",
       "2                Random Forest         0.7625          0.7793       0.7625   \n",
       "6           K Nearest Neighbor         0.6950          0.6989       0.6950   \n",
       "\n",
       "  f1_score  \n",
       "5   0.8397  \n",
       "3   0.8300  \n",
       "4   0.8174  \n",
       "0   0.7967  \n",
       "1   0.7818  \n",
       "2   0.7589  \n",
       "6   0.6935  "
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "X_new = tfidf_vect.transform(test_new['tweets_clean'].astype('str')) # text features\n",
    "X_fe = test_new[fe_cols].to_numpy()\n",
    "X_new = hstack([X_new, X_fe])\n",
    "y_new = test_new['label'].values # target\n",
    "\n",
    "#Function to get the scores for each model in a df\n",
    "def model_score_df(model_dict):   \n",
    "    model_name, ac_score_list, p_score_list, r_score_list, f1_score_list = [], [], [], [], []\n",
    "    for k,v in model_dict.items():   \n",
    "        model_name.append(k)\n",
    "        v.fit(X, y)\n",
    "        y_pred = v.predict(X_new)\n",
    "        ac_score_list.append(accuracy_score(y_new, y_pred))\n",
    "        p_score_list.append(precision_score(y_new, y_pred, average='macro'))\n",
    "        r_score_list.append(recall_score(y_new, y_pred, average='macro'))\n",
    "        f1_score_list.append(f1_score(y_new, y_pred, average='macro'))\n",
    "        model_comparison_df = pd.DataFrame([model_name, ac_score_list, p_score_list, r_score_list, f1_score_list]).T\n",
    "        model_comparison_df.columns = ['model_name', 'accuracy_score', 'precision_score', 'recall_score', 'f1_score']\n",
    "        model_comparison_df = model_comparison_df.sort_values(by='f1_score', ascending=False)\n",
    "    return model_comparison_df\n",
    "\n",
    "model_score_df(model_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model_name</th>\n",
       "      <th>accuracy_score</th>\n",
       "      <th>precision_score</th>\n",
       "      <th>recall_score</th>\n",
       "      <th>f1_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>XGBoost</td>\n",
       "      <td>0.8425</td>\n",
       "      <td>0.8427</td>\n",
       "      <td>0.8425</td>\n",
       "      <td>0.8425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AdaBoost</td>\n",
       "      <td>0.8300</td>\n",
       "      <td>0.8305</td>\n",
       "      <td>0.8300</td>\n",
       "      <td>0.8299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.7625</td>\n",
       "      <td>0.7729</td>\n",
       "      <td>0.7625</td>\n",
       "      <td>0.7602</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            model_name accuracy_score precision_score recall_score f1_score\n",
       "2              XGBoost         0.8425          0.8427       0.8425   0.8425\n",
       "1             AdaBoost         0.8300          0.8305       0.8300   0.8299\n",
       "0  Logistic Regression         0.7625          0.7729       0.7625   0.7602"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Creating a dict of the best models\n",
    "best_model_dict = {'Logistic Regression': LogisticRegression(solver='liblinear', C=0.1, penalty='l2', random_state=3),\n",
    "              'AdaBoost': AdaBoostClassifier(n_estimators=500, random_state=3),\n",
    "              'XGBoost': XGBClassifier(learning_rate=0.5, n_estimater=75, max_depth=3, objective='binary:logistic', random_state=3)}\n",
    "\n",
    "model_score_df(best_model_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test On New Dataset Test2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_2_depressed = pd.read_csv('test_depressed_new_2.csv', index_col=0)\n",
    "test_2_depressed['label'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweets</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>李佳琦是不是在我购物车和收藏夹里装了摄像头  翻到一张要素过多的截图  又买到一条合适舒服的...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>生活碎片（1）  我对Yeezy是真的一直爱不起来完全没有想上脚的欲望…  为什么每次别人找...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>机票又便宜了我不管不管人事调动多频繁 会不会影响到我出去玩的计划6月我该去成都还去哈哈哈哈哈...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>抑郁症 最近做梦总是在挨骂，被亲近的人骂被不亲近的人骂，被认识的人骂被不认识的人骂，我记不清...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>#三星堆遗址考古重大发现# 啊哈哈哈哈哈 三星堆发现新的文物 直接把三叔干上了热门 你们是存...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              tweets  label\n",
       "0  李佳琦是不是在我购物车和收藏夹里装了摄像头  翻到一张要素过多的截图  又买到一条合适舒服的...      1\n",
       "1  生活碎片（1）  我对Yeezy是真的一直爱不起来完全没有想上脚的欲望…  为什么每次别人找...      1\n",
       "2  机票又便宜了我不管不管人事调动多频繁 会不会影响到我出去玩的计划6月我该去成都还去哈哈哈哈哈...      1\n",
       "3  抑郁症 最近做梦总是在挨骂，被亲近的人骂被不亲近的人骂，被认识的人骂被不认识的人骂，我记不清...      1\n",
       "4  #三星堆遗址考古重大发现# 啊哈哈哈哈哈 三星堆发现新的文物 直接把三叔干上了热门 你们是存...      1"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_2_depressed.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweets</th>\n",
       "      <th>label</th>\n",
       "      <th>tweets_chinese</th>\n",
       "      <th>tweets_cut</th>\n",
       "      <th>tweets_clean</th>\n",
       "      <th>n</th>\n",
       "      <th>f</th>\n",
       "      <th>s</th>\n",
       "      <th>t</th>\n",
       "      <th>nr</th>\n",
       "      <th>ns</th>\n",
       "      <th>nt</th>\n",
       "      <th>nw</th>\n",
       "      <th>nz</th>\n",
       "      <th>v</th>\n",
       "      <th>vd</th>\n",
       "      <th>vn</th>\n",
       "      <th>a</th>\n",
       "      <th>ad</th>\n",
       "      <th>an</th>\n",
       "      <th>d</th>\n",
       "      <th>m</th>\n",
       "      <th>q</th>\n",
       "      <th>r</th>\n",
       "      <th>p</th>\n",
       "      <th>c</th>\n",
       "      <th>u</th>\n",
       "      <th>xc</th>\n",
       "      <th>w</th>\n",
       "      <th>PER</th>\n",
       "      <th>LOC</th>\n",
       "      <th>ORG</th>\n",
       "      <th>TIME</th>\n",
       "      <th>O</th>\n",
       "      <th>n_pos</th>\n",
       "      <th>n_neg</th>\n",
       "      <th>word_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>李佳琦是不是在我购物车和收藏夹里装了摄像头  翻到一张要素过多的截图  又买到一条合适舒服的...</td>\n",
       "      <td>1</td>\n",
       "      <td>李佳琦是不是在我购物车和收藏夹里装了摄像头  翻到一张要素过多的截图  又买到一条合适舒服的...</td>\n",
       "      <td>李佳琦 是不是 在 我 购物车 和 收藏夹 里 装 了 摄像头    翻 到 一张 要素 过...</td>\n",
       "      <td>李佳琦 是不是 购物车 收藏夹 里 装 摄像头    翻 一张 要素 过多 截图    买到...</td>\n",
       "      <td>0.1491</td>\n",
       "      <td>0.0118</td>\n",
       "      <td>0.0059</td>\n",
       "      <td>0.0033</td>\n",
       "      <td>0.0802</td>\n",
       "      <td>0.0007</td>\n",
       "      <td>0.0112</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0125</td>\n",
       "      <td>0.2838</td>\n",
       "      <td>0.0020</td>\n",
       "      <td>0.0085</td>\n",
       "      <td>0.0493</td>\n",
       "      <td>0.0099</td>\n",
       "      <td>0.0059</td>\n",
       "      <td>0.0618</td>\n",
       "      <td>0.0164</td>\n",
       "      <td>0.0026</td>\n",
       "      <td>0.0966</td>\n",
       "      <td>0.0256</td>\n",
       "      <td>0.0204</td>\n",
       "      <td>0.0940</td>\n",
       "      <td>0.0171</td>\n",
       "      <td>0.0013</td>\n",
       "      <td>0.0085</td>\n",
       "      <td>0.0020</td>\n",
       "      <td>0.0007</td>\n",
       "      <td>0.0191</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0894</td>\n",
       "      <td>0.3035</td>\n",
       "      <td>1522</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>生活碎片（1）  我对Yeezy是真的一直爱不起来完全没有想上脚的欲望…  为什么每次别人找...</td>\n",
       "      <td>1</td>\n",
       "      <td>生活碎片     我对     是真的一直爱不起来完全没有想上脚的欲望   为什么每次别人找...</td>\n",
       "      <td>生活 碎片       我 对       是 真 的 一直 爱不起来 完全 没有 想 上 ...</td>\n",
       "      <td>生活 碎片             真 一直 爱不起来 完全 没有 想 脚 欲望     每...</td>\n",
       "      <td>0.1705</td>\n",
       "      <td>0.0127</td>\n",
       "      <td>0.0048</td>\n",
       "      <td>0.0020</td>\n",
       "      <td>0.0479</td>\n",
       "      <td>0.0003</td>\n",
       "      <td>0.0054</td>\n",
       "      <td>0.0017</td>\n",
       "      <td>0.0107</td>\n",
       "      <td>0.2849</td>\n",
       "      <td>0.0008</td>\n",
       "      <td>0.0096</td>\n",
       "      <td>0.0558</td>\n",
       "      <td>0.0099</td>\n",
       "      <td>0.0028</td>\n",
       "      <td>0.0873</td>\n",
       "      <td>0.0217</td>\n",
       "      <td>0.0073</td>\n",
       "      <td>0.0682</td>\n",
       "      <td>0.0279</td>\n",
       "      <td>0.0144</td>\n",
       "      <td>0.0950</td>\n",
       "      <td>0.0155</td>\n",
       "      <td>0.0008</td>\n",
       "      <td>0.0031</td>\n",
       "      <td>0.0056</td>\n",
       "      <td>0.0014</td>\n",
       "      <td>0.0321</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0882</td>\n",
       "      <td>0.3364</td>\n",
       "      <td>3549</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>机票又便宜了我不管不管人事调动多频繁 会不会影响到我出去玩的计划6月我该去成都还去哈哈哈哈哈...</td>\n",
       "      <td>1</td>\n",
       "      <td>机票又便宜了我不管不管人事调动多频繁 会不会影响到我出去玩的计划 月我该去成都还去哈哈哈哈哈...</td>\n",
       "      <td>机票 又 便宜 了 我 不管 不管 人事 调动 多 频繁   会不会 影响 到 我 出去玩 ...</td>\n",
       "      <td>机票 便宜 人事 调动 频繁   会不会 影响 出去玩 计划  成都 还去   人事变动 频...</td>\n",
       "      <td>0.1364</td>\n",
       "      <td>0.0128</td>\n",
       "      <td>0.0059</td>\n",
       "      <td>0.0037</td>\n",
       "      <td>0.0302</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0017</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0044</td>\n",
       "      <td>0.3073</td>\n",
       "      <td>0.0004</td>\n",
       "      <td>0.0068</td>\n",
       "      <td>0.0533</td>\n",
       "      <td>0.0059</td>\n",
       "      <td>0.0033</td>\n",
       "      <td>0.0966</td>\n",
       "      <td>0.0154</td>\n",
       "      <td>0.0048</td>\n",
       "      <td>0.1046</td>\n",
       "      <td>0.0233</td>\n",
       "      <td>0.0283</td>\n",
       "      <td>0.0931</td>\n",
       "      <td>0.0292</td>\n",
       "      <td>0.0017</td>\n",
       "      <td>0.0065</td>\n",
       "      <td>0.0039</td>\n",
       "      <td>0.0004</td>\n",
       "      <td>0.0200</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0555</td>\n",
       "      <td>0.2980</td>\n",
       "      <td>5402</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>抑郁症 最近做梦总是在挨骂，被亲近的人骂被不亲近的人骂，被认识的人骂被不认识的人骂，我记不清...</td>\n",
       "      <td>1</td>\n",
       "      <td>抑郁症 最近做梦总是在挨骂 被亲近的人骂被不亲近的人骂 被认识的人骂被不认识的人骂 我记不清...</td>\n",
       "      <td>抑郁症   最近 做梦 总是 在 挨骂   被 亲近 的 人 骂 被 不 亲近 的 人 骂 ...</td>\n",
       "      <td>抑郁症   最近 做梦 总是 挨骂   亲近 骂 亲近 骂   认识 骂 认识 骂   记 ...</td>\n",
       "      <td>0.1691</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0048</td>\n",
       "      <td>0.0048</td>\n",
       "      <td>0.0290</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0290</td>\n",
       "      <td>0.3333</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0145</td>\n",
       "      <td>0.0242</td>\n",
       "      <td>0.0048</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0870</td>\n",
       "      <td>0.0242</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0870</td>\n",
       "      <td>0.0242</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0676</td>\n",
       "      <td>0.0290</td>\n",
       "      <td>0.0048</td>\n",
       "      <td>0.0386</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0242</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.1401</td>\n",
       "      <td>0.2705</td>\n",
       "      <td>207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>#三星堆遗址考古重大发现# 啊哈哈哈哈哈 三星堆发现新的文物 直接把三叔干上了热门 你们是存...</td>\n",
       "      <td>1</td>\n",
       "      <td>三星堆遗址考古重大发现  啊哈哈哈哈哈 三星堆发现新的文物 直接把三叔干上了热门 你们是存...</td>\n",
       "      <td>三星堆遗址 考古 重大 发现    啊 哈哈 哈哈 哈  三星堆 发现 新 的 文物   直...</td>\n",
       "      <td>三星堆遗址 考古 重大 发现     三星堆 发现 新 文物   直接 三叔 干 热门  存...</td>\n",
       "      <td>0.1474</td>\n",
       "      <td>0.0147</td>\n",
       "      <td>0.0028</td>\n",
       "      <td>0.0022</td>\n",
       "      <td>0.0592</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0081</td>\n",
       "      <td>0.0016</td>\n",
       "      <td>0.0197</td>\n",
       "      <td>0.2607</td>\n",
       "      <td>0.0006</td>\n",
       "      <td>0.0056</td>\n",
       "      <td>0.0523</td>\n",
       "      <td>0.0050</td>\n",
       "      <td>0.0019</td>\n",
       "      <td>0.0551</td>\n",
       "      <td>0.0166</td>\n",
       "      <td>0.0053</td>\n",
       "      <td>0.0977</td>\n",
       "      <td>0.0194</td>\n",
       "      <td>0.0110</td>\n",
       "      <td>0.1139</td>\n",
       "      <td>0.0410</td>\n",
       "      <td>0.0009</td>\n",
       "      <td>0.0354</td>\n",
       "      <td>0.0022</td>\n",
       "      <td>0.0034</td>\n",
       "      <td>0.0163</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0895</td>\n",
       "      <td>0.2776</td>\n",
       "      <td>3195</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              tweets  label  \\\n",
       "0  李佳琦是不是在我购物车和收藏夹里装了摄像头  翻到一张要素过多的截图  又买到一条合适舒服的...      1   \n",
       "1  生活碎片（1）  我对Yeezy是真的一直爱不起来完全没有想上脚的欲望…  为什么每次别人找...      1   \n",
       "2  机票又便宜了我不管不管人事调动多频繁 会不会影响到我出去玩的计划6月我该去成都还去哈哈哈哈哈...      1   \n",
       "3  抑郁症 最近做梦总是在挨骂，被亲近的人骂被不亲近的人骂，被认识的人骂被不认识的人骂，我记不清...      1   \n",
       "4  #三星堆遗址考古重大发现# 啊哈哈哈哈哈 三星堆发现新的文物 直接把三叔干上了热门 你们是存...      1   \n",
       "\n",
       "                                      tweets_chinese  \\\n",
       "0  李佳琦是不是在我购物车和收藏夹里装了摄像头  翻到一张要素过多的截图  又买到一条合适舒服的...   \n",
       "1  生活碎片     我对     是真的一直爱不起来完全没有想上脚的欲望   为什么每次别人找...   \n",
       "2  机票又便宜了我不管不管人事调动多频繁 会不会影响到我出去玩的计划 月我该去成都还去哈哈哈哈哈...   \n",
       "3  抑郁症 最近做梦总是在挨骂 被亲近的人骂被不亲近的人骂 被认识的人骂被不认识的人骂 我记不清...   \n",
       "4   三星堆遗址考古重大发现  啊哈哈哈哈哈 三星堆发现新的文物 直接把三叔干上了热门 你们是存...   \n",
       "\n",
       "                                          tweets_cut  \\\n",
       "0  李佳琦 是不是 在 我 购物车 和 收藏夹 里 装 了 摄像头    翻 到 一张 要素 过...   \n",
       "1  生活 碎片       我 对       是 真 的 一直 爱不起来 完全 没有 想 上 ...   \n",
       "2  机票 又 便宜 了 我 不管 不管 人事 调动 多 频繁   会不会 影响 到 我 出去玩 ...   \n",
       "3  抑郁症   最近 做梦 总是 在 挨骂   被 亲近 的 人 骂 被 不 亲近 的 人 骂 ...   \n",
       "4  三星堆遗址 考古 重大 发现    啊 哈哈 哈哈 哈  三星堆 发现 新 的 文物   直...   \n",
       "\n",
       "                                        tweets_clean      n      f      s  \\\n",
       "0  李佳琦 是不是 购物车 收藏夹 里 装 摄像头    翻 一张 要素 过多 截图    买到... 0.1491 0.0118 0.0059   \n",
       "1  生活 碎片             真 一直 爱不起来 完全 没有 想 脚 欲望     每... 0.1705 0.0127 0.0048   \n",
       "2  机票 便宜 人事 调动 频繁   会不会 影响 出去玩 计划  成都 还去   人事变动 频... 0.1364 0.0128 0.0059   \n",
       "3  抑郁症   最近 做梦 总是 挨骂   亲近 骂 亲近 骂   认识 骂 认识 骂   记 ... 0.1691 0.0000 0.0048   \n",
       "4  三星堆遗址 考古 重大 发现     三星堆 发现 新 文物   直接 三叔 干 热门  存... 0.1474 0.0147 0.0028   \n",
       "\n",
       "       t     nr     ns     nt     nw     nz      v     vd     vn      a  \\\n",
       "0 0.0033 0.0802 0.0007 0.0112 0.0000 0.0125 0.2838 0.0020 0.0085 0.0493   \n",
       "1 0.0020 0.0479 0.0003 0.0054 0.0017 0.0107 0.2849 0.0008 0.0096 0.0558   \n",
       "2 0.0037 0.0302 0.0000 0.0017 0.0000 0.0044 0.3073 0.0004 0.0068 0.0533   \n",
       "3 0.0048 0.0290 0.0000 0.0000 0.0000 0.0290 0.3333 0.0000 0.0145 0.0242   \n",
       "4 0.0022 0.0592 0.0000 0.0081 0.0016 0.0197 0.2607 0.0006 0.0056 0.0523   \n",
       "\n",
       "      ad     an      d      m      q      r      p      c      u     xc  \\\n",
       "0 0.0099 0.0059 0.0618 0.0164 0.0026 0.0966 0.0256 0.0204 0.0940 0.0171   \n",
       "1 0.0099 0.0028 0.0873 0.0217 0.0073 0.0682 0.0279 0.0144 0.0950 0.0155   \n",
       "2 0.0059 0.0033 0.0966 0.0154 0.0048 0.1046 0.0233 0.0283 0.0931 0.0292   \n",
       "3 0.0048 0.0000 0.0870 0.0242 0.0000 0.0870 0.0242 0.0000 0.0676 0.0290   \n",
       "4 0.0050 0.0019 0.0551 0.0166 0.0053 0.0977 0.0194 0.0110 0.1139 0.0410   \n",
       "\n",
       "       w    PER    LOC    ORG   TIME      O  n_pos  n_neg  word_count  \n",
       "0 0.0013 0.0085 0.0020 0.0007 0.0191 0.0000 0.0894 0.3035        1522  \n",
       "1 0.0008 0.0031 0.0056 0.0014 0.0321 0.0000 0.0882 0.3364        3549  \n",
       "2 0.0017 0.0065 0.0039 0.0004 0.0200 0.0000 0.0555 0.2980        5402  \n",
       "3 0.0048 0.0386 0.0000 0.0000 0.0242 0.0000 0.1401 0.2705         207  \n",
       "4 0.0009 0.0354 0.0022 0.0034 0.0163 0.0000 0.0895 0.2776        3195  "
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_2_depressed['tweets_chinese'] = test_2_depressed['tweets'].apply(lambda tweets: format_str(tweets))\n",
    "  \n",
    "test_2_depressed['tweets_cut'] = test_2_depressed['tweets_chinese'].apply(lambda t: ' '.join(list(jieba.cut(t, \n",
    "                                                                                 use_paddle=True\n",
    "#                                                                                  cut_all=True\n",
    "                                                                                ))))\n",
    "test_2_depressed['tweets_clean'] = test_2_depressed['tweets_cut'].apply(lambda t: ' '.join([word for word in t.split(' ') if word not in STOPWORDS ]))\n",
    "\n",
    "test_2_depressed = generate_pos(test_2_depressed)\n",
    "test_2_depressed = generate_sentiment(test_2_depressed)\n",
    "\n",
    "test_2_depressed['word_count'] = test_2_depressed.iloc[:, -31:-2].sum(axis=1)\n",
    "for col in fe_cols:\n",
    "    test_2_depressed[col] = test_2_depressed[col]/test_2_depressed['word_count']\n",
    "\n",
    "test_2_depressed.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 37)"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_2_depressed.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model_name</th>\n",
       "      <th>accuracy_score</th>\n",
       "      <th>precision_score</th>\n",
       "      <th>recall_score</th>\n",
       "      <th>f1_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AdaBoost</td>\n",
       "      <td>0.6200</td>\n",
       "      <td>0.5000</td>\n",
       "      <td>0.3100</td>\n",
       "      <td>0.3827</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>XGBoost</td>\n",
       "      <td>0.6100</td>\n",
       "      <td>0.5000</td>\n",
       "      <td>0.3050</td>\n",
       "      <td>0.3789</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.4600</td>\n",
       "      <td>0.5000</td>\n",
       "      <td>0.2300</td>\n",
       "      <td>0.3151</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            model_name accuracy_score precision_score recall_score f1_score\n",
       "1             AdaBoost         0.6200          0.5000       0.3100   0.3827\n",
       "2              XGBoost         0.6100          0.5000       0.3050   0.3789\n",
       "0  Logistic Regression         0.4600          0.5000       0.2300   0.3151"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Creating a dict of the best models\n",
    "best_model_dict = {'Logistic Regression': LogisticRegression(solver='liblinear', C=0.1, penalty='l2', random_state=3),\n",
    "              'AdaBoost': AdaBoostClassifier(n_estimators=500, random_state=3),\n",
    "              'XGBoost': XGBClassifier(learning_rate=0.5, n_estimater=75, max_depth=3, objective='binary:logistic', random_state=3)}\n",
    "\n",
    "X_new = tfidf_vect.transform(test_2_depressed['tweets_clean'].astype('str')) # text features\n",
    "X_fe = test_2_depressed[fe_cols].to_numpy()\n",
    "X_new = hstack([X_new, X_fe])\n",
    "y_new = test_2_depressed['label'].values # target\n",
    "\n",
    "#Function to get the scores for each model in a df\n",
    "def model_score_df(model_dict):   \n",
    "    model_name, ac_score_list, p_score_list, r_score_list, f1_score_list = [], [], [], [], []\n",
    "    for k,v in model_dict.items():   \n",
    "        model_name.append(k)\n",
    "        v.fit(X, y)\n",
    "        y_pred = v.predict(X_new)\n",
    "        ac_score_list.append(accuracy_score(y_new, y_pred))\n",
    "        p_score_list.append(precision_score(y_new, y_pred, average='macro'))\n",
    "        r_score_list.append(recall_score(y_new, y_pred, average='macro'))\n",
    "        f1_score_list.append(f1_score(y_new, y_pred, average='macro'))\n",
    "        model_comparison_df = pd.DataFrame([model_name, ac_score_list, p_score_list, r_score_list, f1_score_list]).T\n",
    "        model_comparison_df.columns = ['model_name', 'accuracy_score', 'precision_score', 'recall_score', 'f1_score']\n",
    "        model_comparison_df = model_comparison_df.sort_values(by='f1_score', ascending=False)\n",
    "    return model_comparison_df\n",
    "\n",
    "model_score_df(best_model_dict)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
